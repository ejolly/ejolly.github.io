<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://eshinjolly.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://eshinjolly.com/" rel="alternate" type="text/html" /><updated>2024-01-04T20:24:35-05:00</updated><id>https://eshinjolly.com/feed.xml</id><title type="html">Eshin Jolly</title><entry><title type="html">Reproducible scientific Python environments with conda</title><link href="https://eshinjolly.com/2022/10/15/conda-environments/" rel="alternate" type="text/html" title="Reproducible scientific Python environments with conda" /><published>2022-10-15T00:00:00-04:00</published><updated>2022-10-15T00:00:00-04:00</updated><id>https://eshinjolly.com/2022/10/15/conda-environments</id><content type="html" xml:base="https://eshinjolly.com/2022/10/15/conda-environments/"><![CDATA[<p>This is a brief explanation of a workflow that I’ve been using for research/data-science projects in Python. It makes use of <code class="language-plaintext highlighter-rouge">conda</code> environments <strong>co-located</strong> with project files. This meets several key criteria that I was looking for:</p>

<ul>
  <li>Environments are easily recreatable, meaning less worry about ever borking things</li>
  <li>Reproducible workspace across different machines</li>
  <li>Dependencies placed under version control for open-science and collaboration</li>
  <li>“Portable” environments that are easy to move around like normal folders</li>
</ul>

<p>Below are the key steps to use this setup.</p>

<p class="note"><strong>Note</strong> This post replaces an earlier version I was drafting using <a href="https://www.craft.do/">Craft</a>. I took down that post from this site, but you can still access the draft version <a href="https://www.craft.do/s/0MKzYVAYgRan2x">here</a>. The commands below were run on macOS and should be relatively similar to other Nix-y systems (e.g. Ubuntu and Windows subsystem for Linux).</p>

<h1 id="setting-up-anaconda-or-miniconda">Setting up Anaconda or Miniconda</h1>

<p>You can use <a href="https://docs.conda.io/en/latest/miniconda.html">this link</a> to grab the latest Miniconda (on macOS you want the <code class="language-plaintext highlighter-rouge">bash</code> script). Or you can use the installation the link at the <a href="https://www.anaconda.com/products/individual">bottom of this page</a> to download Anaconda instead. Miniconda is a bit faster cause it’s more bare-bones, but Anaconda includes some default packages. Once you’ve downloaded either of those files you’ll need to open up a Terminal and <code class="language-plaintext highlighter-rouge">cd</code> to the location of the file (probably <code class="language-plaintext highlighter-rouge">Downloads</code> or <code class="language-plaintext highlighter-rouge">Desktop</code>). From there you’ll need to run the installer and follow the prompts, which you can do by typing <code class="language-plaintext highlighter-rouge">bash fileYouDownloaded.sh</code>.</p>

<p>You can verify the installation worked by asking your system what Python it sees now using <code class="language-plaintext highlighter-rouge">which python</code>. If everything worked it should point to the Python installed inside your <code class="language-plaintext highlighter-rouge">anaconda3</code> or <code class="language-plaintext highlighter-rouge">miniconda3</code> directory.</p>

<h1 id="creating-a-new-environment-for-each-project">Creating a new environment for each project</h1>

<p>Most guides will tell you to create a new <em>named</em> environment using the <code class="language-plaintext highlighter-rouge">-n/--name</code> flag to <code class="language-plaintext highlighter-rouge">conda create</code>. But a more reproducible setup is to create a <em>local</em> environment within your project folder. Let’s say you have a project folder called <code class="language-plaintext highlighter-rouge">myproject/</code>. The command below creates a new environment in a sub-directory called <code class="language-plaintext highlighter-rouge">env</code>. It installs Python 3.8, <code class="language-plaintext highlighter-rouge">pip</code> (for libraries on <a href="https://pypi.org/">pypi</a>), and specifically uses the <a href="https://conda-forge.org/">conda-forge</a> channel for grabbing them:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># From within myproject/</span>
conda create <span class="nt">-p</span> ./env <span class="nv">python</span><span class="o">=</span>3.8 pip <span class="nt">-c</span> conda-forge
</code></pre></div></div>
<p class="caption">Change your Python version to what makes most sense for your project. You can also omit <code class="language-plaintext highlighter-rouge">-c conda-forge</code> to just install from the normal <code class="language-plaintext highlighter-rouge">defaults</code> channel.</p>

<p>You should now see a new <code class="language-plaintext highlighter-rouge">env/</code> directory. You can activate the environment by pointing to it: <code class="language-plaintext highlighter-rouge">conda activate ./env</code>. You <strong>should not</strong> commit this folder to version control as it can be quite large depending on how complex your project requirements get. So make sure to <code class="language-plaintext highlighter-rouge">echo 'env/' &gt;&gt; .gitignore</code>.</p>

<h1 id="backing-up-and-restoring-your-environment">Backing up and restoring your environment</h1>

<p>This is the critical piece that makes this setup work: the <code class="language-plaintext highlighter-rouge">environment.yml</code> file. This file is a <em>recipe</em> for rebuilding your environment in a platform-independent way. To create this recipe run the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Export the environment recipe to a file called environment.yml</span>
conda <span class="nb">env export</span> <span class="nt">--no-builds</span> <span class="nt">-f</span> environment.yml
</code></pre></div></div>
<p class="caption">The <code class="language-plaintext highlighter-rouge">--no-builds</code> flag exports the current environment in a platform-independent way. This means you should be able to use the same <code class="language-plaintext highlighter-rouge">environment.yml</code> across different operating systems (e.g. macOS, Windows, etc)</p>

<p>You should <strong>rerun this command any time you install or uninstall new libraries and packages</strong>. You should also commit those changes to version control:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git add environment.yml
git commit <span class="nt">-m</span> <span class="s2">"saved environment"</span>
</code></pre></div></div>

<p>To restore an environment from this file (e.g. if you or a collaborator are working on another machine or you break something) just do:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Make sure the env isn't active </span>
conda deactivate
<span class="c"># Delete the env folder</span>
<span class="nb">rm</span> <span class="nt">-r</span> <span class="nb">env</span>
<span class="c"># Create a new env using the spec in environment.yml</span>
conda <span class="nb">env </span>create <span class="nt">-p</span> ./env <span class="nt">-f</span> environment.yml
</code></pre></div></div>

<p>You can also make sure your environment is in-sync with <code class="language-plaintext highlighter-rouge">environment.yml</code> by running the following command which will install and uninstall dependencies as needed:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">env </span>update <span class="nt">-p</span> ./env <span class="nt">-f</span> environment.yml <span class="nt">--prune</span>
</code></pre></div></div>

<h1 id="a-few-closing-suggestions">A few closing suggestions</h1>

<h2 id="make-sure-you-know-what-environment-is-active-before-running-conda-install-commands">Make sure you know what environment is active before running conda install commands</h2>

<p>Make sure whenever you want to add or remove package to a project environment (e.g. <code class="language-plaintext highlighter-rouge">myproject</code> ), you <code class="language-plaintext highlighter-rouge">conda activate ./env</code> it first. If you don’t, you’ll accidentally be adding and removing things from your <code class="language-plaintext highlighter-rouge">base</code> environment!</p>

<h2 id="be-careful-about-mixing-and-matching-conda-channels">Be careful about mixing and matching conda channels!</h2>

<p>In snipped above I used the <code class="language-plaintext highlighter-rouge">-c</code> flag to <code class="language-plaintext highlighter-rouge">conda install</code> from the <code class="language-plaintext highlighter-rouge">conda-forge</code> channel. By default <code class="language-plaintext highlighter-rouge">conda</code> will install from the <code class="language-plaintext highlighter-rouge">defaults</code> channel which points to <a href="https://anaconda.org/">anaconda.org</a>, whereas <code class="language-plaintext highlighter-rouge">conda-forge</code> points to <a href="https://conda-forge.org/">conda-forge.org</a>.</p>

<p>In general, you can save yourself a lot of headaches by simply <strong>sticking with the same channel for installing everything.</strong> For example if I installed <a href="https://numpy.org/">numpy</a> using <code class="language-plaintext highlighter-rouge">conda install -c conda-forge numpy</code>, then it’s a good idea to <strong>keep using</strong> <code class="language-plaintext highlighter-rouge">conda-forge</code> for other packages I want like <code class="language-plaintext highlighter-rouge">pandas</code>: <code class="language-plaintext highlighter-rouge">conda install -c conda-forge pandas</code>, rather than <code class="language-plaintext highlighter-rouge">conda install pandas</code>, which is equivalent to <code class="language-plaintext highlighter-rouge">conda install -c defaults pandas</code>.</p>

<p>There might be times when you can’t avoid mixing and matching, but it’s a good heuristic to help avoid the dreaded “environment conflict” messages that you might encounter otherwise. I have been a long time <code class="language-plaintext highlighter-rouge">defaults</code> user because of one or two less than pleasant experiences with <code class="language-plaintext highlighter-rouge">conda-forge</code> several years ago. Plus there used to be a significant speed difference when using the Intel compiled MKL (Math Kernel Libraries) and the open BLAS (Basic Linear Algebra Subprograms) that power libraries like <code class="language-plaintext highlighter-rouge">numpy</code>. But lately most of that seems to have changed and for the last few projects I’ve been exclusively preferring <code class="language-plaintext highlighter-rouge">conda-forge</code> for its breadth and especially for any <a href="https://eshinjolly.com/pymer4">R related dependencies</a>.</p>

<h2 id="optional-extra-automation">Optional extra automation</h2>

<p>If you’re interested in automating this workflow a bit, I made a few bash functions and aliases that you should be able to drop in to a <code class="language-plaintext highlighter-rouge">.zshrc</code> or <code class="language-plaintext highlighter-rouge">.bashrc</code> file. Specifically:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">envinit()</code> for creating a brand new <code class="language-plaintext highlighter-rouge">env/</code> with a few basic libs and exporting its <code class="language-plaintext highlighter-rouge">environment.yml</code></li>
  <li><code class="language-plaintext highlighter-rouge">envsave()</code> which exports the current <code class="language-plaintext highlighter-rouge">env</code> to <code class="language-plaintext highlighter-rouge">environment.yml</code> and also appends <code class="language-plaintext highlighter-rouge">pip</code> packages installed from something other than <a href="https://pypi.org/">pypi</a>, e.g. github or locally from source. That’s because currently conda <a href="https://github.com/conda/conda/issues/9308">doesn’t install these packages</a> when recreating an environment. So you’ll have to install them manually with <code class="language-plaintext highlighter-rouge">pip</code></li>
  <li><code class="language-plaintext highlighter-rouge">envcheck()</code> which checks if <code class="language-plaintext highlighter-rouge">envsave</code> needs to be run and does so. Useful as a <a href="https://www.atlassian.com/git/tutorials/git-hooks#conceptual-overview">git pre-commit hook</a>.</li>
  <li><code class="language-plaintext highlighter-rouge">envactivate()</code> which basically “overloads” <code class="language-plaintext highlighter-rouge">conda activate</code> to prefer a <code class="language-plaintext highlighter-rouge">./env</code> if one exists in the current directory</li>
  <li><code class="language-plaintext highlighter-rouge">newproject.py</code> a python script that bootstraps a folder structure I often use while also setting up a <a href="https://code.visualstudio.com/docs/getstarted/settings#_workspace-settings">VSCode Workspace</a> and git repo</li>
</ul>

<p>When starting a new project I’ll usually do something like this:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create new project scaffold using the `newproject` script at the end of this post</span>
newproject <span class="nt">--name</span> coolscience

<span class="c"># Create a new conda env using an alias for the `envinit()` bash function</span>
<span class="nb">cd </span>coolscience
ie

<span class="c"># Setup up verison control for env</span>
<span class="nb">echo</span> <span class="s1">'env'</span> <span class="o">&gt;&gt;</span> .gitignore
git add environment.yml
git commit <span class="nt">-m</span> <span class="s2">"saved environment"</span>
</code></pre></div></div>

<p>This creates the following project structure and gives me a Python environment ready to go with some reasonable defaults and editor setup:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># contents of coolscience/</span>
|-- analysis/
|-- code/
|-- data/
|-- docs/
|-- <span class="nb">env</span>/                  <span class="c"># actual env contents</span>
|-- figures/
|-- presentations/
|-- LICENSE
|-- README.md
|-- environment.yml       <span class="c"># env recipe</span>
|-- .vscode/
|-- .gitignore
</code></pre></div></div>

<p>As I continue working I’ll do the following:</p>
<ul>
  <li>whenever I first <code class="language-plaintext highlighter-rouge">cd</code> into <code class="language-plaintext highlighter-rouge">coolscience</code> I’ll use <code class="language-plaintext highlighter-rouge">ca</code> to activate the environment installed in <code class="language-plaintext highlighter-rouge">coolscience/env</code></li>
  <li>I’ll <code class="language-plaintext highlighter-rouge">ce</code> to check if my <code class="language-plaintext highlighter-rouge">environment.yml</code> needs to be updated with any new packages I <code class="language-plaintext highlighter-rouge">conda install</code>-ed or removed</li>
  <li>If I remember I’ll use <code class="language-plaintext highlighter-rouge">se</code> whenever I <code class="language-plaintext highlighter-rouge">conda install</code> something. But honestly I forget a lot so end up using <code class="language-plaintext highlighter-rouge">ce</code></li>
</ul>

<p class="note"><strong>Note</strong> As mentioned above the <code class="language-plaintext highlighter-rouge">environment.yml</code> generated by <code class="language-plaintext highlighter-rouge">envsave()</code> and <code class="language-plaintext highlighter-rouge">envcheck()</code> will also output  installed <code class="language-plaintext highlighter-rouge">pip</code> packages for convenience. However, any pip packages installed from source or after <code class="language-plaintext highlighter-rouge">git clone</code>-ing something will need to be reinstalled manually whenever you recreate the environment. The aliases will print out a message indicating whenever you’re in that situation.</p>

<p>And the aliases/functions/scripts are here:</p>

<div class="not-prose gist-wrapper">
<script src="https://gist.github.com/ejolly/6c2dcf4b7a1ce39121859770a4f5a5d2.js"></script>
</div>

<div class="not-prose gist-wrapper">
    <script src="https://gist.github.com/ejolly/0f448085a35540ab42dfd5cec1d9ce19.js"></script>
</div>]]></content><author><name></name></author><category term="scientific tooling" /><category term="python" /><category term="anaconda" /><summary type="html"><![CDATA[How to setup anaconda and create portable project environments]]></summary></entry><entry><title type="html">Sometimes a project can be an unexpected but delightful journey</title><link href="https://eshinjolly.com/2021/05/03/delightful-journey/" rel="alternate" type="text/html" title="Sometimes a project can be an unexpected but delightful journey" /><published>2021-05-03T00:00:00-04:00</published><updated>2021-05-03T00:00:00-04:00</updated><id>https://eshinjolly.com/2021/05/03/delightful-journey</id><content type="html" xml:base="https://eshinjolly.com/2021/05/03/delightful-journey/"><![CDATA[<p><em>This post is a reflection on my experiences as a grad-student and the unexpected ways that a <a href="https://www.sciencedirect.com/science/article/pii/S0960982221004632">recently published project</a> impacted my life. I wrote this as a current post-doc looking back, and I have no expectations this will generalize to everyone (or anyone) in similar circumstances, but maybe some folks might it useful or entertaining.</em></p>

<h2 id="the-hamster-wheel-death-spiral">The hamster-wheel death-spiral</h2>

<p class="fig"><img src="https://images.squarespace-cdn.com/content/v1/59bc9a021f318d2ce39268c8/1506233183117-B968XGVTX2WMGDZ7SQ4R/ke17ZwdGBToddI8pDm48kPPJlLwfF1ZKCaao6uPxYBJ7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0k6sq9GEl9ZUDkp1sRKcAyKq73LBCgj7GHSm2wXSrBl1oXcWlnrWoBiy1_RNtaPhhA/101+-+Searching.jpg?format=2500w" alt="" /></p>

<p class="caption-centered">Inspecting his fridge for something to eat, the grad student peers into a box as barren as his stomach, heart, and wallet<sup id="algs"><a href="#flgs">*</a></sup></p>

<p>You don’t have to doom-scroll for long on Twitter to get a sense for how graduate school (and academia writ large) can wear away at even the most excited or resilient folks. While academia suffers from many of the same systemic cultural issues that affect other parts of society, there’s also the long, pseudo-predictable delay between the effort you put into a project and the traditional “outcome,” i.e. publication; kind of like a variable-interval reward-schedule <a href="https://www.google.com/books/edition/Behavior_Modification_Principles_and_Pro/8TZBBAAAQBAJ?hl=en&amp;gbpv=1&amp;dq=Behavior+Modification:+Principles+and+Procedures+%7C+6th+Edition&amp;pg=PR4&amp;printsec=frontcover">(Miltenberger, 1996)</a>. I’m certainly no exception and <a href="https://www.sciencedirect.com/science/article/pii/S0960982221004632">my recently published work on the social value and function of gossip</a> has been one of the longest roller coasters of my academic life.</p>

<p>I entered graduate school in 2012 and quickly came to learn, both implicitly and explicitly (like many folks I assume), that even though some things are changing, there’s a very real sense in which publications feel like the “currency” of academia. Walking in as a bright-eyed student, it’s not unreasonable to eventually come to fatalistically measure your own sense of progress, development, and even self-worth through the “success” of your science, rather than the <em>joy</em> of your discoveries and contributions, no matter how big or small. Each scathing review, rejected manuscript, and ambiguous statistical result<sup id="a1"><a href="#f1">1</a></sup> can feel like a huge setback and make you question: Why am I doing this again? What’s the point? Does anyone care? Do <em>I</em> care?</p>

<p>Rather than rehash the details of my particular flavor of misery, suffice to say for a long time in grad school, it was this way of looking at the enterprise and myself that consumed my thoughts. It reinforced the hamster-wheel death-spiral that takes a serious toll on mental health: I felt the pressure of the <a href="https://www.theguardian.com/science/2011/sep/05/publish-perish-peer-review-science">publish-or-perish</a> machine, which made me question <em>why</em> I was doing what I was doing, which destroyed my motivation to participate in science, which only served to remind me of the pressure. I constantly felt lost, defeated, and angry. My cynicism and negativity grew about many things, but mostly the <em>process</em> of doing science, and anything I was actually interested in studying suffered as a result. As the late <a href="https://scholar.harvard.edu/files/dwegner/files/wegner_premature_demise_1992.pdf">Dan Wegner (1992)</a> characterized it, I became increasingly comfortable as a <em>pointer</em>: quick to criticize, but reluctant to do anything about it. At peak negativity, I called my <a href="https://jihiijolly.com/">sister</a> and exasperated loudly:</p>

<blockquote>
  <p>The problem <em>with</em> science, is that it’s done by humans.</p>
</blockquote>

<p>She lovingly had a coffee mug made for me with a minor, but purposeful typo that still annoys (and yet provokes) me to this day.</p>

<p class="fig"><img src="/assets/images/journey/mug.png" alt="" /></p>

<p class="caption-centered">(Fig 1) No, no, no. The problem OF science, is that it’s done by humans.</p>

<h2 id="and-so-it-begins">And so it begins…</h2>

<p>When I started working with my then recently hired <a href="http://lukejchang.com/">advisor</a> (~2015), not only was I coming from a pretty dismal <em>mental</em> place, I was very much lacking the technical competency that our study ultimately required to pull off. We were both interested in understanding why gossip seems so ubiquitous <a href="https://www.google.com/books/edition/Grooming_Gossip_and_the_Evolution_of_Lan/nN5DFNT-6ToC?hl=en&amp;gbpv=1&amp;pg=PA1&amp;printsec=frontcover">(Dunbar, 1996)</a> and what social function(s) it serves. We began by piloting <em>group</em> experiments in a laboratory setting, because we were very committed to fostering <em>real</em> interactions between people. As life would have it, when you live and work in <a href="https://pbs.dartmouth.edu/">The Shire</a> with a small student body that’s mostly away during summers, scheduling multi-person studies can be a bit of a… challenge<sup id="a2"><a href="#f2">2</a></sup>. If my memory serves, over many months, we collected a grand total of about 4 groups (of 6 people) worth of useable pilot data (~24) that wasn’t otherwise plagued by technical or scheduling issues. Not a great start, but a great hamper on enthusiasm.</p>

<p>Around the same time, my advisor shared with me how he had recently learned enough web-development to create our <a href="https://www.cosanlab.com">lab website</a> using Python and the neat things that might be possible with web technology. Little did I know that these initial conversations would drastically change my approach to not only this particular project, but my thinking about research more broadly. Having no experience in this area, we started pondering what a real-time interactive group experiment <em>online</em> might look like. We came across the <a href="https://www.meteor.com/">MeteorJS</a> framework: a JavaScript library that makes is relatively easy to “synchronize” data between the end-users of a web application in near real-time. While in some respects this type of functionality seems trivial in 2021 (e.g. real-time chat applications are everywhere now and even embedded directly into many websites), to a grad-student with the tiniest amount of web development experience, it felt like opening the door to an entire world of things that I hadn’t ever noticed before.</p>

<p>Rather than suggest I “learn enough to get the project going,” I was fortunate to be mentored by someone who encouraged me to jump head-first into any number of rabbit holes, no matter how directly or in-directly related to my work. Gifted a small supply of Red Bull, I threw myself headfirst into the wild west of web-app development and <a href="https://www.destroyallsoftware.com/talks/wat">JavaScript</a>.</p>

<p class="fig"><img src="https://ars.els-cdn.com/content/image/1-s2.0-S1097276509006418-gr2_lrg.jpg" alt="" /></p>

<p class="caption-centered">(Fig 2) In Alon’s (2009) <a href="https://www.sciencedirect.com/science/article/pii/S1097276509006418">How to Choose a Good Scientific Problem</a> he encourages PIs to adopt the mentorship schema on the right, suggesting that meandering is an integral part of the research process rather than a nuisance. He suggests that sailing into the unknown can facilitate more rich, strange and unique experiences and combat the emotional toll of perceived failure that comes from expecting the trajectory on the left. However, embracing meandering, Alon notes, requires an “uncommon degree of openness.” In my experience, this can be especially true the farther you meander from where you think you should be going.</p>

<h2 id="someone-was-right-its-about-the-journey-not-the-destination">Someone was right: it’s about the journey not the destination</h2>

<p class="fig"><img src="https://images.squarespace-cdn.com/content/v1/59bc9a021f318d2ce39268c8/1536166062949-ZKW14T8AMSRU9YYBZ1VR/ke17ZwdGBToddI8pDm48kLkXF2pIyv_F2eUT9F60jBl7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0iyqMbMesKd95J-X4EagrgU9L3Sa3U8cogeb0tjXbfawd0urKshkc5MgdBeJmALQKw/191+-+Enjoying+%28smaller%29.jpg?format=2500w" alt="" /></p>

<p class="caption-centered">Enjoying his work, the grad student solemnly ponders whether he is a victim of Stockholm Syndrome<sup id="algs"><a href="#flgs">*</a></sup></p>

<p>Not only did I find myself becoming as interested in web development as our original research question, but in hindsight, it’s the <em>way</em> it unfolded that had a huge impact on me. Rather than learning a new skill as a “means to an end,” I found myself amused at the irony of intently reading a large literature on human communication and coordination on the one hand, while trying to engineer a system that allowed <em>machines</em> (internet browsers) to communicate and coordinate on the other. My amusement grew into obsession, the kind of obsession you’re told is an implicit “requirement” for a successful research career. But mine wasn’t really about a scientific topic per se. Rather, it was learning to <em>think differently about what was possible</em> as a budding theoretician and experimentalist. Of finding inspiration in unexpected places.</p>

<p>Over many caffeine-induced manic sessions I came across ideas and concepts that were completely new to me. Take for example, the difference between a top-to-bottom, procedurally executed experiment script designed for a single participant and one that needs to manage the large number of potential <em>states</em> that a group of interacting participants could find themselves at any moment:</p>
<ul>
  <li>What should happen to everyone else if one person drops out part way through a study?</li>
  <li>What should happen if someone refreshes their browser during the study, quickly disconnecting and reconnecting to their fellow group members?</li>
  <li>What if someone stops responding to their group members, because their cat runs out of their house and they have to chase it down the street because well, <em>life</em>? (this actually happened to one of our participants who emailed me apologizing profusely and who was obviously compensated in full)</li>
</ul>

<p>It turns out that <a href="https://en.wikipedia.org/wiki/Finite-state_machine">finite-state machines</a> are a useful abstraction for handling exactly this type of complexity:</p>

<p class="fig"><img src="https://turkserver.readthedocs.io/en/latest/_images/experiment-flow.png" alt="" /></p>

<p class="caption-centered">(Fig 3) An early version of a state diagram I made to track a participant’s “flow” through our study. The hand-drawn version on my office white-board along with my lack of hygiene at the time, looked a lot like a scene from <a href="https://i.pinimg.com/originals/d5/1a/43/d51a434cc8daa460a700a3a3d4b5d402.jpg">A Beautiful Mind</a> minus any semblance of genius.</p>

<p>Little did I know that ideas like this would serve as a kind of mental and conceptual “<a href="https://en.wikipedia.org/wiki/Zone_of_proximal_development">scaffold</a>” for contributing to later work in the lab, such as how <a href="https://advances.sciencemag.org/content/7/17/eabf7129">state dynamics in the ventromedial prefrontal cortex seem to reflect on-going emotional experiences</a>. I came to see this project as an opportunity to <em>actually</em> practice the technical skills I had read about, and that I (much) later had the honor of teaching to <em>other</em> folks, e.g. <a href="https://eshinjolly.com/2019/01/04/git_github/">version control via git and github</a><sup id="a3"><a href="#f3">3</a></sup>. Bolstered by tiny bits of confidence and enthusiasm (or just sheer mania) I began asking a few questions to the author of one of the software libraries that my project used. I ended up contributing to <em>their</em> <a href="https://turkserver.readthedocs.io/en/latest/launching/live-management.html#troubleshooting">documentation</a> based on my own experiences running my study and was encouraged to apply for a PhD internship at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-new-york/">Microsoft Research NYC</a><sup id="a4"><a href="#f4">4</a></sup>.</p>

<p>I was immediately confused as to why that made any sense and what a cognitive neuroscience PhD student with mediocre programming ability had to offer a bunch of computer scientists. After many pep talks about why it was worth applying regardless of the outcome, I feel like I lucked out again getting an opportunity to spend a summer working with <a href="https://www.sidsuri.com/">Sid Suri</a> and <a href="https://www.asc.upenn.edu/people/faculty/duncan-watts-phd">Duncan Watts</a> in 2016. Not only did that turn out to be one of the most fun experiences of my graduate career, but it helped me discover an entire field I knew very little about at the time: computational social science <a href="https://science.sciencemag.org/content/369/6507/1060">(Lazar et al., 2020)</a>. I attended a <a href="https://www.kellogg.northwestern.edu/news-events/conference/ic2s2/2016.aspx">conference</a> I had never been to before, in a city that I had never visited, and coaxed some lab members to make the trek as well.</p>

<p>One of the most profound things I learned from my experience at the time and that has stuck with me ever since, was the <em>ethnography</em> of the Mturk marketplace. Workers are <a href="https://dl.acm.org/doi/pdf/10.1145/2872427.2883036">highly communicative</a> and <a href="https://dl.acm.org/doi/pdf/10.1145/2818048.2819942">highly collaborative</a>, which honestly should come as no surprise to anyone. But it often does so long as the primary way researchers think of Mturk is as “cheap labour force” rather than <a href="https://ghostwork.info/">ordinary folks just trying to get by, by grinding through hours of surveys and experiments each day</a>. This was consistent with the participant feedback I received in our study (snippets from <a href="https://turkopticon.ucsd.edu/">turkopticon</a>):</p>

<blockquote>
  <p><em>Very fun game. I think it was 30 minutes at most, and I got a $3 bonus plus the $1.5 base pay. Very glad to be grouped with awesome cooperative people, and the chatting was fun too. Definitely one of the best HITs I’ve done.</em></p>
</blockquote>

<p>It turns out that people <em>really like</em> interacting with other people online and some of our findings demonstrating how gossip can build social connections, extended well beyond the end of our study:</p>

<blockquote>
  <p><em>Purple birdie, if you are reading this - it was nice chatting with you. Thanks for liking my bee, but your bird was still better!</em></p>
</blockquote>

<h2 id="back-to-the-churn-but-somehow-different">Back to The Churn, but somehow different</h2>

<p>The several years that followed my internship, weren’t particularly kind to my gossip project. It languished in peer-review hell cycling through 6 different journals with a combined total of 13 rounds of review. Sometimes it was rejected with no feedback, because “it’s not possible to study gossip empirically,” (thanks Reviewer 2!). More often, it was because notions about <em>what gossip is</em> elicited strong reactions, given our strong folk intuitions <a href="https://journals.sagepub.com/doi/abs/10.1037/1089-2680.8.2.78">(Foster, 2004)</a>. I happen to like this quote by <a href="https://journals.sagepub.com/doi/abs/10.1037/1089-2680.8.2.138?journalCode=rgpa">Paul Bloom (2003)</a>:</p>

<blockquote>
  <p><em>…gossip is an arena where some of the most interesting aspects of our mental life are laid bare.</em></p>
</blockquote>

<p>All the while at various conferences that I presented my work, I often received the exact <em>opposite</em> feedback. Though the project certainly hadn’t solved one of the “great mysteries of the world,” I felt encouraged when it received positive feedback and raised novel questions and ideas in the folks I shared it with. In reconciling these two contrasting sets of experiences, I often found myself slipping back into familiar mental territory: See? No one <em>actually</em> cares. This is all just a waste of time. I heard the squeaking of the hamster-wheel and wondered if I would ever get off.</p>

<p>But in an unexpected about-face of my mental life, my <a href="https://www.nytimes.com/2003/09/07/magazine/the-futile-pursuit-of-happiness.html">“psychological immune system”</a> seemed to slowly to kick-in. It reminded me of how unexpected this ride had been thus far, and how the <em>journey</em> that I’m writing about here, is what I would ultimately remember and cherish most, <em>not</em> the outcome<sup id="a5"><a href="#f5">5</a></sup>. This way of thinking once again slowly started to save me from myself. And each time it did, I got excited about <a href="https://eshinjolly.com/pymer4">building</a> or <a href="https://codeforuv.org/">contributing</a> to something new. And each time I did that, I discovered a new way to think about my work and my interests, and <a href="https://eshinjolly.com/2020/07/12/scipy/">a new way to think about software development</a> that focused on helping others <em>learn</em> and <em>build intuitions</em> as critical pieces of “optimal” software design.</p>

<p>Receiving even the smallest piece of positive feedback from a student or independent researcher outside of the lab, about the utility <a href="https://github.com/cosanlab">our open-source tools</a> offered their own work or how any of my research has influenced <em>their</em> thinking, warms my heart in a way that no amount of publications or career in science ever could. I am only ever reminded of how the kindness of strangers (and friends) helped <em>me</em> and influenced <em>my</em> thinking. The notion that I could have even a modicum of that kind of impact on <em>someone else</em> as I’ve been bumbling about all these years, has always left me in disbelief.</p>

<p>I’ve always loved the <em>social</em> side of the scientific enterprise, and in my mind it’s not made up of publication records and tenure track job markets, but rather the surprising and unexpected ways that we can shape each other’s thinking and perspective of the world.</p>

<h2 id="you-are-in-the-right-place-at-the-right-time">You <em>are</em> in the right place at the right time</h2>

<p>The <a href="https://socialaffectiveneuro.org/">Social Affective Neuroscience Society’s</a> 2021 virtual conference wrapped up this past weekend and one of my personal highlights was a lovely distinguished scholars Q&amp;A session with venerable <a href="https://www.wikiwand.com/en/Chris_Frith">Chris</a> and <a href="https://www.wikiwand.com/en/Uta_Frith">Uta Frith</a><sup id="a6"><a href="#f6">6</a></sup>. When asked a question about what drove their own interests, research program, and ultimately careers, and any advice they had for aspiring researchers, they both humbly discussed the outsized role of luck. Chris Frith cheekily joked: “Maybe the advice to young scientists is to be in the right place at the right time.” Uta Frith, however was quick to correct him stating:</p>

<blockquote>
  <p>“No I think the advice is that you <em>are</em> in the right place at the right time. Just be open to what is around you and what the ideas are and go with them.”</p>
</blockquote>

<p>It’s only in hindsight that I realize how much I unconsciously took this advice to heart all these years. Unbeknownst to me at the time, I’ve tried to reframe each project setback or negative experience in graduate school as some kind of new opportunity to learn and grow. To find inspiration in unexpected places. Outcomes be damned. Do I have more faith that maybe <a href="https://thehardestscience.com/2016/08/11/everything-is-fucked-the-syllabus/">everything <em>isn’t</em> fucked</a>? Not necessarily, but I do have more faith in my own ability to seek out the little joys that make the ride worth staying on a bit longer. Maybe this is because I was <a href="https://buddhistsolutions.transistor.fm/">raised (but don’t actively practice) Buddhist</a>. Or maybe it’s because after years of pointing, <a href="https://scholar.harvard.edu/files/dwegner/files/wegner_premature_demise_1992.pdf">bumbling</a> just turns out to be more fun. Or far more likely, maybe it’s because I’ve just been extremely lucky. Regardless, if I could say something to a younger version of myself it would be a paraphrase of Uta Frith’s advice:</p>

<blockquote>
  <p>If you treat each project as an opportunity to <em>grow as an individual</em> instead of an opportunity to <em>grow your career</em>, it’ll not only help you survive grad school, but it could very well change the direction of your life in delightful and unexpected ways.</p>
</blockquote>

<p>And if you’re <em>really</em> lucky, Jimmy Fallon might use your work as the setup to a mediocre joke:</p>

<iframe class="mx-auto sm:h-[400px] sm:w-[600px]" src="https://www.youtube.com/embed/W4ARRNm4Idk?start=278" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<hr />

<p><b id="flgs">* </b><em>Photo and caption credits go to the ever cathartic<a href="https://brickademics.com"> Lego grad student</a><a href="#algs">↩</a></em></p>

<p><b id="f1">1. </b><em>I’m absolutely not advocating for defining project success based on statistical significance. However, we most often discuss the impact of the academic <a href="https://arxiv.org/abs/physics/9909033">file drawer problem</a> in terms of the quality of findings in a field. But on a more personal level, and especially as a trainee, until the measure of a “productive scientist” grows to include more than the publication of significant findings (e.g. null results, research tooling, outreach, etc) there’s still a sense of defeat or despair that grows when a years-long project “doesn’t work” or “isn’t publishable” because of a lack of a significant finding.</em><a href="#a1">↩</a></p>

<p><b id="f2">2. </b><em>That’s not to say it’s impossible to recruit for group studies in a smaller student population. Just ask <a href="https://twitter.com/ErikNook/status/1383090788171669504?s=20">Emma Templeton</a> who has been far more persistent and successful than me. </em><a href="#a2">↩</a></p>

<p><b id="f3">3. </b><em>Thanks to git/github (of which I knew essentially nothing when this journey began), I tracked down my first commit on this project to <a href="https://github.com/cosanlab/PGG_meteor/commit/89298f2">July 23, 2015</a>, almost 6 years ago as of this post.</em><a href="#a3">↩</a></p>

<p><b id="f4">4. </b><em> If your program, financial, and personal situation allows it, I highly recommend applying for PhD internships as a graduate student coming from a psych/neuro PhD program. Not only is it experientially useful to see what “something else” looks like, so you can better make decisions about the future of your own life, but you may be surprised about the way it comes around to influence your thinking about current projects. If you can make it work, don’t let your advisor talk you out of it. And no, you don’t need to be any kind of technical or coding expert. </em><a href="#a4">↩</a></p>

<p><b id="f5">5. </b><em> I’m not ignorant of the fact publishing your work in a “high quality” outlet certainly feels like an accomplishment in a way that publishing in another outlet might not. But in all honesty the story in this post is one that I’ve long been waiting to share and one I’ve been planning to share regardless of where this work was ultimately ended up. It’s the unexpected journey rather than the destination that has personally impacted me the most.</em><a href="#a5">↩</a></p>

<p><b id="f6">6. </b><em> The Friths announced they are writing a graphic novel about mentalizing that I intend to purchase and read as soon as it’s out, and put up on my shelf right next to <a href="https://www.wikiwand.com/en/The_Little_Prince">The Little Prince.</a> </em><a href="#a6">↩</a></p>]]></content><author><name></name></author><category term="academia" /><category term="javascript" /><category term="web development" /><category term="scientific tooling" /><category term="conference" /><summary type="html"><![CDATA[The project taketh, but the project also giveth.]]></summary></entry><entry><title type="html">Managing Amazon’s Mechanical Turk with a nice user interface</title><link href="https://eshinjolly.com/2020/10/15/svelteturk/" rel="alternate" type="text/html" title="Managing Amazon’s Mechanical Turk with a nice user interface" /><published>2020-10-15T00:00:00-04:00</published><updated>2020-10-15T00:00:00-04:00</updated><id>https://eshinjolly.com/2020/10/15/svelteturk</id><content type="html" xml:base="https://eshinjolly.com/2020/10/15/svelteturk/"><![CDATA[<p>In the <a href="https://www.cosanlab.com">Cosan Lab</a>, like many other reserach groups, we often run experiments or surveys on the web using <a href="https://www.mturk.com/">Amazon’s Mechanical Turk marketplace</a>. One of the constant pain points we’ve run into is on the <em>administration</em> side of things. In other words, quickly and reliably creating and managing HITs and contacting and paying Workers, which Mturk’s ownsite is lackluster for (when using custom URLs). There already exist several great libraries like <a href="https://psiturk.org/">psiturk</a> which act as a stort of “one-stop-shop” for building, managing and hosting web studies. However, we’ve often found that it’s helpful to decouple administration from the tasks themselves such as in the case of <a href="https://www.sciencedirect.com/science/article/pii/S0960982221004632">multiplayer synchronous experiments</a>. While there are several great software libraries for programmatically controlling Mturk as a requestor such as <a href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/mturk.html">Boto</a> in Python, this can be a bit of a chore and still presents a hurdle for less experienced or younger students (e.g. undergraduates) in the lab who were less comfortable with programming.</p>

<p>To help out with this I started developing a graphical desktop application (GUI) to make life a bit easier. Its working name is <a href="https://eshinjolly.com/svelteturk/#">SvelteTurk</a>, because I’ve fallen absolutely in love with a Javascript framework called <a href="https://svelte.dev/">Svelte</a>. What attracted me to Svelte was not only it’s ease of use and <a href="https://eshinjolly.com/2020/07/12/scipy">intuition-building design</a>, but that it was, and is, explicitly used in a context where not every team member has to be highly technically proficient: editorial teams at the The New York Times. Many of their latest interactive graphics <a href="https://www.nytimes.com/interactive/2021/us/covid-cases.html">especially around coronavirus</a>, are built and embedded alongside content using Svelte. Further, the original author, <a href="https://twitter.com/Rich_Harris">Rich Harris</a> is a very compelling and thought-provoking speaker when it comes to <a href="https://www.youtube.com/watch?v=AdNJ3fydeao">rethinking the approach to modern web development</a>.</p>

<p>Combined with another framework called <a href="https://www.electronjs.org/">Electron</a> which makes it relatively straighforward to build cross-platform desktop applications using the languages of the web (HTML, CSS, JS), Svelteturk allows you to create, manage, review, and edit MTurk HITs, interact with Workers, and handle Qualifications, without writing a single line of code or using Mturk’s clunky web interface. It’s <em>very much</em> a work-in-progress and I’ve only been handling bug-fixes or adding new features as needed internally. However, it’s entirely <a href="https://github.com/ejolly/svelteturk">open source</a> and as always contributions are welcome! Clicking the image below will take you to the documentation site that has a ton more information.</p>

<div style="text-align:center">
<a href="https://eshinjolly.com/svelteturk">
  <img class="mx-auto" src="https://eshinjolly.com/svelteturk/assets/setup.jpg" width="600" />
  </a>
  </div>
<figcaption style="top:0"><em>SvelteTurk only handles nodes and connections highlighted in green, namely the creation and management of HITs via communication with MTurk and a local database. It's up to you to build your experiment or survey and store appropriate data as see you see fit.</em></figcaption>]]></content><author><name></name></author><category term="scientific-tooling" /><category term="svelte" /><category term="javascript" /><category term="web development" /><category term="project" /><summary type="html"><![CDATA[Giving Mturk the paint job it needs]]></summary></entry><entry><title type="html">Scipy 2020</title><link href="https://eshinjolly.com/2020/07/12/scipy/" rel="alternate" type="text/html" title="Scipy 2020" /><published>2020-07-12T00:00:00-04:00</published><updated>2020-07-12T00:00:00-04:00</updated><id>https://eshinjolly.com/2020/07/12/scipy</id><content type="html" xml:base="https://eshinjolly.com/2020/07/12/scipy/"><![CDATA[<p>This was the first time I attended and presented at the annual <a href="https://conference.scipy.org/">Scientific Python Conference</a>, an interesting mix of folks from academic, commercial, and governmental organizations, all brought together through a common interest in scientific-computing with Python. My presentation was focused on the design choices we made in the <a href="https://www.cosanlab.org">Cosan Lab</a> around building <a href="https://nltools.org">Nltools</a> our open-source toolbox for analyzing neuroimaging data. The key point of what I was hoping to get across was thinking about the design of scientific software from the perspective of the end-user’s <em>learning experience</em> and <em>intuition-building</em>.</p>

<p>Rather that rehash the main points here, I’ve embedded the video in case it’s of interest.</p>

<iframe class="mx-auto sm:h-[400px] sm:w-[600px]" src="https://www.youtube.com/embed/1c1AnXLs7xM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

<p>What was notable to me, other than the warm, fun, and welcoming community was how several speakers made <a href="https://www.youtube.com/watch?v=nxXr0LNdQUU&amp;feature=emb_logo">related points around software design in this particular frame</a>, e.g.</p>

<div class="w-3/4 mx-auto text-center">
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Three rules for building good software:<br /><br />1. build on existing tools wherever you can<br />2. the development team should be the first users<br />3. iterate until onboarding new users is painless<a href="https://twitter.com/DrAnneCarpenter?ref_src=twsrc%5Etfw">@DrAnneCarpenter</a> at <a href="https://twitter.com/hashtag/SciPy2020?src=hash&amp;ref_src=twsrc%5Etfw">#SciPy2020</a> <a href="https://t.co/ps9YcH0ybe">pic.twitter.com/ps9YcH0ybe</a></p>&mdash; Dillon Niederhut PhD (@dillonniederhut) <a href="https://twitter.com/dillonniederhut/status/1281308571733495809?ref_src=twsrc%5Etfw">July 9, 2020</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<p>All-in-all, I highly recommend that psych/neuro folks try to engage with communities like SciPy if you get a chance. There’s a lot of amazing work and opportunities to contribute and give back to the free and open tooling we in turn rely upon for our work.</p>]]></content><author><name></name></author><category term="conference" /><category term="python" /><category term="scientific-tooling" /><summary type="html"><![CDATA[Annual Scientific Computing with Python Conference]]></summary></entry><entry><title type="html">Comparing common analysis strategies for repeated measures data</title><link href="https://eshinjolly.com/2019/02/18/rep_measures/" rel="alternate" type="text/html" title="Comparing common analysis strategies for repeated measures data" /><published>2019-02-18T00:00:00-05:00</published><updated>2019-02-18T00:00:00-05:00</updated><id>https://eshinjolly.com/2019/02/18/rep_measures</id><content type="html" xml:base="https://eshinjolly.com/2019/02/18/rep_measures/"><![CDATA[<h1 id="what-is-this-all-about">What is this all about?</h1>

<p>My hope with this post is to provide a <em>conceptual</em> overview of how to deal with a specific type of dataset commonly encountered in the social sciences (and very common in my own disciplines of experimental psychology and cognitive neuroscience). My goal is not to provide mathematical formalisms, but rather build some intuitions and try to avoid as much jargon as possible. Specifically, I’d like to compare some of the more common analysis strategies one can use and how they vary by situation, ending with some takeaways that hopefully guide your future decision-making. But to do so we need to start at the beginning…</p>

<h1 id="repeated-what">Repeated what?</h1>

<p>Datasets come in all different shapes and sizes. In many introductory tutorials, classes, and even real world examples, folks are usually dealing with datasets that are referred to as satisfying the “i.i.d assumption” of many common statistical models. What does this mean in English? It refers to the fact that each data-point is largely independent of other data points in the complete dataset. More specifically, it means that the <em>residuals</em> of a model (i.e. what’s left over that the model can’t explain) are independent of each other and that they all come from the <em>same</em> distribution which has a mean of 0 and a standard deviation of \(\sigma^2\). In other words, knowing something about one error that the model makes tells you little about any other error the model makes, and by extension, knowing something about one data point tells you little about any other datapoint.</p>

<p>However, many types of data contain “repeats” or “replicates” such as measuring the same people over time or under different conditions. These data notably  <em>violate</em> this assumption. In these cases, some data points <em>are</em> more similar to each other than other data points. Violations of these assumptions can lead to model estimates that are not as accurate as they could possibly be (<a href="https://pdfs.semanticscholar.org/fe95/9879dbf2b06f33b9ef07b67897135be57abf.pdf">Ugrinowitsch et al, 2004</a>). The more insidious issue is that inferences made using these estimates (e.g. computing t-statistics and by extension p-values) can be wildly inaccurate and produce <a href="https://www.wikiwand.com/en/False_positives_and_false_negatives">false-positives</a> (<a href="https://www.ncbi.nlm.nih.gov/pubmed/3615759">Vasey &amp; Thayer, 1987</a>). Let’s try to make this more concrete by considering two different datasets.</p>

<p><img src="/assets/images/compare_rfx/2019-02-18-compare_rfxlm_robustlm_and_lmm_scatter.png" alt="png" /></p>

<p>In case 1 (left) we give 21 people a survey 1 time each and try to see if their survey responses share any relationship with some demographic about them. 21 total data points, pretty straightforward. In case 2 (right), we give 3 people a survey 7 times each and do the same thing. 21 total data points again, but this time each data point is not independent of every other. In the first case, each survey response is independent of any other. That is, knowing something about one person’s response tells you little about another person’s response. However, in the second case this is not true. Knowing something about person A’s survey response the first time you survey them tells you a bit more about person A’s survey response the second time you survey them, whereas it does necessarily give you more information about any of person B’s responses. Hence the non-independence. In the most extreme case estimating a model ignoring these dependencies in the data can completely reverse the resulting estimates, a phenomenon known as <a href="https://www.wikiwand.com/en/Simpson%27s_paradox">Simpson’s Paradox</a>.</p>

<h1 id="analysis-strategies">Analysis Strategies.</h1>

<p>So what do we typically do? Well there are a few different analysis “traditions” that have dealt with this in different ways. This isn’t by any means an exhaustive list, but approaches that are reasonably common across many different literatures.</p>

<h2 id="multi-level-models">Multi-level models</h2>

<p>Like many other researchers in psychology/neuroscience, I was first taught that repeated-measures ANOVAs are the only way to analyze these type of data. However, this has fallen a bit out of practice in favor of the more flexible approach of multi-level/mixed effects modeling (<a href="https://www.sciencedirect.com/science/article/pii/S0749596X07001398">Baayen et al, 2008</a>). I don’t want to focus on why multi-level modeling is often far more preferable, as that’s a different discussion (e.g. better handling of missing data, different numbers of repeats, additional levels of replicates, different numbers of replicates, etc), but suffice to say that it once you start using this approach there’s essentially no reason to ever run a repeated measured ANOVA again. Going into all the details of how these models work is beyond the scope of this post, but I’ll link to a few resources throughout. Conceptually, multi-level modeling simultaneously estimates coefficients that describe a relationship across the entire dataset, as well as within each group of replicates. In our example above, this amounts to estimating the relationship between survey responses and demographics for the entire population of survey respondents, but also the degree to which individual people deviate from these estimates. This has the net effect of “pooling” estimates and their associated errors together and works in a manner not entirely unlike using a prior if you are familiar with Bayesian terminology or regularization/smoothing if machine-learning is more your thing. The result of estimating a model this way means that estimates can “help each other out” such that we can impute values if some of our survey respondents didn’t fill out the survey each time we asked them to, or we can “clean-up” noisy estimates we get from specific individuals by assuming that individuals’ estimates all come from the same population, thereby restricting wonky values they may take on.</p>

<p>In practice using these models can be a bit tricky. This is due to the fact that it’s not immediately obvious how to set these models up for estimation. For example, should we assume that each respondent has a different relationship between their survey results and demographics? Or should we simply assume that their survey results differ on average but vary with their demographics in the same way? Specifically, users have a variety of choices for how to specify what’s referred to as the “random effects” (deviation estimates) part of the model. You may have come across terminology like “random intercepts” or “random slopes.” In our example, this is the difference between allowing a model to learn a unique mean estimate for each individual’s survey responses and learning a unique regression estimate for the relationship between each individual’s survey responses and demographic outcome measure. In many cases, computing the complete set of coefficients one <em>could</em> compute (intercepts, slopes, and the correlations between them for every predictor) (<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3881361/">Barr, et al, 2013</a>) leads the model to fail to converge, leaving a user with unreliable estimates. This has lead to suggestions to keep models relatively “simple” with respect to the inference one is trying to make (<a href="https://arxiv.org/abs/1506.04967">Bates, et al 2015</a>), or compare different model structures and use a model selection criteria to adjudicate between them <em>before</em> performing inferences (<a href="https://www.sciencedirect.com/science/article/pii/S0749596X17300013">Matuschek et al, 2017</a>). Pretty tricky huh? Try <a href="https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html">this guide</a> to help you out if you venture down this path or check out <a href="https://ourcodingclub.github.io/2017/03/15/mixed-models.html">this post</a> for a nice visual treatment. <a href="https://psycnet.apa.org/record/2017-52405-001">Brauer &amp; Curtin, 2018</a> is a particularly good one-stop-shop for review, theory, practice, estimation issues, and code snippets.  There are a <a href="https://www.jaredknowles.com/journal/2013/11/25/getting-started-with-mixed-effect-models-in-r">ton</a> of <a href="http://www.bodowinter.com/tutorial/bw_LME_tutorial1.pdf">resources</a> <a href="http://www.stat.columbia.edu/~gelman/arm/">available</a> if multi-level models have got you excited.</p>

<h2 id="robustcorrected-standard-errors">Robust/corrected standard errors</h2>

<p>In other academic fields/areas, there is an entirely different tradition for handling these types of data. For example, in some economics disciplines “robust/sandwich/huber-white” standard errors are computed for what is otherwise a standard linear regression model. <a href="http://projects.iq.harvard.edu/files/gov2001/files/sesection_5.pdf">This lecture</a> provides a nice math-ish overview of what these techniques are, but the general takeaway is that this approach entails computing the regression coefficients in a “typical” manner using ordinary least squares (OLS) regression, but “correcting” the variance of these estimators (i.e. the standard errors) for how heteroscedastic they are. That is, how much their variances differ. There are several ways to account for heteroscedasticicty that incorporate things like small-sample and auto-correlation correction, but another one is to compute these robust estimates with respect to “clusters” or grouping factors in the data. In the example above, clusters would comprise survey respondents and each survey response would comprise a data point <em>within</em> that cluster. Therefore, this approach completely ignores the fact that there are repeated measurements when computing the regression coefficients, but takes the repeated measures data into account when making <em>inferences</em> on these coefficients by adjusting their standard errors. For an overview of this calculation see <a href="http://econweb.umd.edu/~sarzosa/teach/2/Disc2_Cluster_handout.pdf">this presentation</a> and for a more formal treatment see <a href="http://cameron.econ.ucdavis.edu/research/Cameron_Miller_JHR_2015_February.pdf">Cameron &amp; Miller, 2015</a>.</p>

<h2 id="two-stage-regressionsummary-statistics-approach">Two-stage-regression/summary statistics approach<sup id="a1"><a href="#f1">*</a></sup></h2>

<p>Finally, a third approach we can use is what has been sometimes referred to as a two-stage-regression (<a href="http://www.stat.columbia.edu/~gelman/research/published/459.pdf">Gelman, 2005</a>) or the summary statistics approach (<a href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.4780111304">Frison &amp; Pocock, 1992</a>; <a href="https://www.fil.ion.ucl.ac.uk/spm/doc/papers/aph_rfx.pdf">Holmes &amp; Friston, 1998</a>). This approach is routine in the analysis of functional MRI data (<a href="https://www.ncbi.nlm.nih.gov/pubmed/19463958/">Mumford &amp; Nichols, 2009</a>). Conceptually, this looks like fitting  a standard OLS regression model to each survey respondent separately, and then fitting a second OLS model to the coefficients from each individual subject’s fit. In the simplest case this equivalent to calculating a one-sample t-test over individuals’ coefficients. You might notice that this approach “feels” similar to the multi-level approach, and in colloquial English, there are in fact multiple levels of modeling going on. However, notice how each first-level model is estimated completely independently of every other model and how their errors or the variance of their estimates are not aggregated in any meaningful way. This means that we lose out on some of the benefits we gain from the formal multi-level modeling framework described above. Yet what we might lose in benefits we gain back in simplicity: there are no additional choices to be made such as choosing an appropriate “random effects” structure. In fact, <a href="http://www.stat.columbia.edu/~gelman/research/published/459.pdf">Gelman, 2005</a> notes that two-stage-regression can be viewed as a special case of multi-level modeling in which we assume that the distribution from which individual/cluster level coefficients comes has infinite variance.</p>

<h1 id="how-do-we-decide">How do we decide?</h1>

<p>Having all these tools at our disposal can sometimes make it tricky to figure out which approach is preferable for what situation and whether there is one approach that is always better than the others (spoiler: there isn’t). To better understand when we might use each approach let’s consider some of the most common situations we might encounter. I’ll refer to these as the “dimensions” along which our data can vary.</p>

<h2 id="dimension-1-sample-size-of-units-we-would-like-to-make-inferences-about">Dimension 1: Sample size of units we would like to make inferences about</h2>

<p>The most common thing that varies about different datasets is simply their size, i.e. how many observations we’re really dealing with. In the case of non-independent data, an analyst may most often be interested in making inferences about a particular “level” of the data. In our survey example, this is generalizing to “people” rather than specific instances of the survey. So this dimension varies based on how many individuals we sampled, irrespective of how many times we sampled any given individual.</p>

<h2 id="dimension-2-sample-size-of-units-nested-within-units-we-would-like-to-make-inferences-about">Dimension 2: Sample size of units nested within units we would like to make inferences about</h2>

<p>Another dimension in which our repeated-measures data may vary, is how many repeats we’re dealing with. In our example above, this is the number of observations we have about any given individual. Did each person fill out the survey 5 times? 10? 100? This dimension therefore varies based on how often we sample any given individual, irrespective of how many total individuals we sample.</p>

<h2 id="dimension-3-variability-between-units-we-would-like-to-make-inferences-about">Dimension 3: Variability between units we would like to make inferences about</h2>

<p>A key way in which each of these analysis approaches varies is how they handle (or don’t) variability between clusters of replicates. In our example above, this is the variance <em>between</em> individuals. Do different people really respond differently from each other? At one extreme we can treat every individual survey response as entirely independent ignoring the fact that we surveyed individuals multiple times and pretending each survey is totally unique. At the other end, we can assume that the relationship between survey responses and demographics come from a higher-level distribution and specific people’s estimates are instances of this distribution, preserving the fact that each person’s own responses are more similar to each other than they are to anyone else’s responses. I’ll return to this a bit more below.</p>

<h1 id="simulations-can-help-us-build-intuitions">Simulations can help us build intuitions.</h1>

<p>Often in cases like this we can use simulated data, designed to vary in particular ways,  to help us gain some insight as to how these things influence our different analysis strategies. So let’s see how that looks. I’m going to be primarily using the <a href="http://eshinjolly.com/pymer4/">pymer4</a> Python package that I wrote to simulate some data and compare these different models. I wrote this package originally so I could reduce the <a href="https://www.apa.org/research/action/multitask">switch cost</a> I kept experiencing bouncing between R and Python for my work. I quickly realized that my primary need for R was using the fantastic <a href="https://cran.r-project.org/web/packages/lme4/index.html">lme4</a> package for multi-level modeling and so I wrote this Python package as a way to use lme4 from within Python while playing nicely with the rest of the scientific Python stack (e.g. pandas, numpy, scipy, etc). Since then the package has grown quite a bit (<a href="http://joss.theoj.org/papers/10.21105/joss.00862">Jolly, 2018</a>), including the ability to fit the different types of models discussed above and simulate different kinds of data. Ok let’s get started:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Import what we need
</span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">pymer4.simulate</span> <span class="kn">import</span> <span class="n">simulate_lmm</span><span class="p">,</span> <span class="n">simulate_lm</span>
<span class="kn">from</span> <span class="nn">pymer4.models</span> <span class="kn">import</span> <span class="n">Lm</span><span class="p">,</span> <span class="n">Lmer</span><span class="p">,</span> <span class="n">Lm2</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="n">sns</span><span class="p">.</span><span class="n">set_context</span><span class="p">(</span><span class="s">'poster'</span><span class="p">);</span>
<span class="n">sns</span><span class="p">.</span><span class="n">set_style</span><span class="p">(</span><span class="s">"whitegrid"</span><span class="p">)</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div></div>

<h2 id="starting-small">Starting small</h2>

<p>Let’s start out with a single simulated dataset and fit each type of model discussed above. Below I’m generating multi-level data similar to our toy example above. The dataset is comprised of 50 “people” with 50 “replicates” each. For each person, we measured 3 independent variables (e.g. 3 survey questions) and would like to relate them to 1 dependent variable (e.g. 1 demographic outcome).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_obs_grp</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">num_grps</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">num_coef</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">formula</span> <span class="o">=</span> <span class="s">'DV ~ IV1 + IV2 + IV3'</span> <span class="c1"># Not required for generating data, but just for convenience estimating models below
</span><span class="n">data</span><span class="p">,</span> <span class="n">blups</span><span class="p">,</span> <span class="n">betas</span> <span class="o">=</span> <span class="n">simulate_lmm</span><span class="p">(</span><span class="n">num_obs_grp</span><span class="p">,</span> <span class="n">num_coef</span><span class="p">,</span> <span class="n">num_grps</span><span class="p">)</span>
<span class="n">data</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div class="dataframe">
<table>
  <thead>
    <tr>
      <th></th>
      <th>DV</th>
      <th>IV1</th>
      <th>IV2</th>
      <th>IV3</th>
      <th>Group</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.962692</td>
      <td>0.884919</td>
      <td>-1.027279</td>
      <td>1.267401</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.692995</td>
      <td>-0.034800</td>
      <td>1.487490</td>
      <td>-0.623623</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.227617</td>
      <td>-0.841247</td>
      <td>0.227976</td>
      <td>-1.411721</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.502931</td>
      <td>1.466788</td>
      <td>-1.332548</td>
      <td>-1.336735</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2.254925</td>
      <td>0.675905</td>
      <td>-0.400129</td>
      <td>0.977755</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>

<p>We can see that the overall dataset is generated as described above. Simulating data this way also allows us to generate the best-linear-unbiased-predictions (<a href="https://www.wikiwand.com/en/Best_linear_unbiased_prediction">BLUPs</a>) for each person in our dataset. These are the coefficients for each individual person.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">blups</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<div class="dataframe">
<table>
  <thead>
    <tr>
      <th></th>
      <th>Intercept</th>
      <th>IV1</th>
      <th>IV2</th>
      <th>IV3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Grp1</th>
      <td>0.334953</td>
      <td>0.804446</td>
      <td>1.273423</td>
      <td>0.484458</td>
    </tr>
    <tr>
      <th>Grp2</th>
      <td>0.296504</td>
      <td>0.533969</td>
      <td>1.499689</td>
      <td>-0.323965</td>
    </tr>
    <tr>
      <th>Grp3</th>
      <td>0.269028</td>
      <td>0.748626</td>
      <td>0.826473</td>
      <td>0.494888</td>
    </tr>
    <tr>
      <th>Grp4</th>
      <td>0.489680</td>
      <td>0.714883</td>
      <td>1.073006</td>
      <td>0.103898</td>
    </tr>
    <tr>
      <th>Grp5</th>
      <td>0.224353</td>
      <td>0.898960</td>
      <td>1.171890</td>
      <td>0.034940</td>
    </tr>
  </tbody>
</table>
</div>

<p>Finally, we can also checkout the “true” coefficients that generated these data. These are the “correct answers” we hope that our models can recover. Since these data have been simulated using the addition of noise to each individual’s data (\((\mu=0,\sigma^2=1)\), and with variance across individuals (pymer4’s default is \(\sigma^2=0.25\)) we don’t expect perfect recovery of these parameters, but something pretty close (we’ll explore this more below).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"True betas: </span><span class="si">{</span><span class="n">betas</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Regression  coefficients for intercept, IV1, IV2, and IV3
True betas: [0.18463772 0.78093358 0.97054762 0.45977883]
</code></pre></div></div>

<h2 id="evaluating-performance">Evaluating performance</h2>

<p>Ok time to evaluate some modeling strategies. For each model type I’ll fit the model to the data as described, and then compute 3 metrics:</p>
<ol>
  <li><strong>Absolute Error of Coefficient Recovery</strong> - this is simply the sum of the absolute value differences between the real coefficients and the estimated ones. It gives us the total error of our model with respect to the data-generating coefficients. We could have computed the average instead of the sum, but since our simulated data are all on the same scale, the sum provides us the exact amount we’re off from what we were expecting to recover.</li>
  <li><strong>Sum of Model Standard Errors</strong> - this and the next measure are more related to the inferences we want to make on our parameters. SE and the associated confidence intervals tell us the total amount of variance around our estimates given this particular modeling strategy. Once again, we could have computed the average, but like above, the sum gives us the total variance across all our parameters.</li>
  <li><strong>Sum of Model T-statistics</strong> - this is the sum of the absolute value of the t-statistics of our model estimates. This gives us a sense of how likely we would be to walk away with the inference that there is a statistically significant relationship between our independent variables and dependent variable. All else being equal, larger t-stats generally mean smaller p-values so we can build an intuition about how sensitive our modeling strategy is to tell us “yup this is a statistically significant effect.”</li>
</ol>

<h3 id="multi-level-models-1">Multi-level models</h3>

<p>Let’s begin with fitting a multi-level model specifying the complete set of all possible parameters we can estimate. This has the effect of letting each individual have their own set of regression estimates while still treating these estimates as coming from a common distribution. You can see below we can recover the parameters pretty well and as we expect all our are results are “significant.”</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Fit lmer with random intercepts, slopes, and their correlations
</span><span class="n">lmer</span> <span class="o">=</span> <span class="n">Lmer</span><span class="p">(</span><span class="n">formula</span> <span class="o">+</span> <span class="s">'+ (IV1 + IV2 + IV3 | Group)'</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="n">lmer</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">summarize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">lmer</span><span class="p">.</span><span class="n">coefs</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Absolute Error of Coef Recovery: </span><span class="si">{</span><span class="n">diffs</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="n">lmer</span><span class="p">.</span><span class="n">coefs</span><span class="p">[</span><span class="s">'Estimate'</span><span class="p">])</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Sum of Model Standard Errors: </span><span class="si">{</span><span class="n">lmer</span><span class="p">.</span><span class="n">coefs</span><span class="p">[</span><span class="s">'SE'</span><span class="p">].</span><span class="nb">sum</span><span class="p">()</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Sum of Model T statistics: </span><span class="si">{</span><span class="n">lmer</span><span class="p">.</span><span class="n">coefs</span><span class="p">[</span><span class="s">'T-stat'</span><span class="p">].</span><span class="nb">abs</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="dataframe">
<table>
  <thead>
    <tr>
      <th></th>
      <th>Estimate</th>
      <th>2.5_ci</th>
      <th>97.5_ci</th>
      <th>SE</th>
      <th>DF</th>
      <th>T-stat</th>
      <th>P-val</th>
      <th>Sig</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>(Intercept)</th>
      <td>0.146146</td>
      <td>0.072889</td>
      <td>0.219402</td>
      <td>0.037377</td>
      <td>49.105624</td>
      <td>3.910072</td>
      <td>2.831981e-04</td>
      <td>***</td>
    </tr>
    <tr>
      <th>IV1</th>
      <td>0.800926</td>
      <td>0.720934</td>
      <td>0.880917</td>
      <td>0.040813</td>
      <td>49.575279</td>
      <td>19.624452</td>
      <td>5.065255e-25</td>
      <td>***</td>
    </tr>
    <tr>
      <th>IV2</th>
      <td>0.964310</td>
      <td>0.874273</td>
      <td>1.054347</td>
      <td>0.045938</td>
      <td>48.977731</td>
      <td>20.991603</td>
      <td>3.900389e-26</td>
      <td>***</td>
    </tr>
    <tr>
      <th>IV3</th>
      <td>0.418673</td>
      <td>0.336092</td>
      <td>0.501255</td>
      <td>0.042134</td>
      <td>49.064194</td>
      <td>9.936621</td>
      <td>2.449657e-13</td>
      <td>***</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Absolute Error of Coef Recovery: 0.10582723675727804
Sum of Model Standard Errors: 0.16626160033359066
Sum of Model T statistics: 54.46274837271574
</code></pre></div></div>

<p>Next, let’s see what happens when we fit a multi-level model with the simplest possible “random effects” structure. Notice that by not letting each individual be free to have their own estimates (aside from their own mean/intercept), our coefficient recovery drops a little bit, but our t-statistics increase dramatically. This looks to be driven by the fact that the variance estimates of the coefficients (standard errors) are quite a bit smaller. All else being equal, we would be much more likely to identify “significant” relationships using a simpler, or in this case, “misspecified” multi-level model, since we know that the data were generated such that each individual did in fact, have different BLUPs.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Fit lmer with random-intercepts only
</span><span class="n">lmer_mis</span> <span class="o">=</span> <span class="n">Lmer</span><span class="p">(</span><span class="n">formula</span> <span class="o">+</span> <span class="s">'+ (1 | Group)'</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="n">lmer_mis</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">summarize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">lmer_mis</span><span class="p">.</span><span class="n">coefs</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Absolute Error of Coef Recovery: </span><span class="si">{</span><span class="n">diffs</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="n">lmer_mis</span><span class="p">.</span><span class="n">coefs</span><span class="p">[</span><span class="s">'Estimate'</span><span class="p">])</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Sum of Model Standard Errors: </span><span class="si">{</span><span class="n">lmer_mis</span><span class="p">.</span><span class="n">coefs</span><span class="p">[</span><span class="s">'SE'</span><span class="p">].</span><span class="nb">sum</span><span class="p">()</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Sum of Model T statistics: </span><span class="si">{</span><span class="n">lmer_mis</span><span class="p">.</span><span class="n">coefs</span><span class="p">[</span><span class="s">'T-stat'</span><span class="p">].</span><span class="nb">abs</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="dataframe">
<table>
  <thead>
    <tr>
      <th></th>
      <th>Estimate</th>
      <th>2.5_ci</th>
      <th>97.5_ci</th>
      <th>SE</th>
      <th>DF</th>
      <th>T-stat</th>
      <th>P-val</th>
      <th>Sig</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>(Intercept)</th>
      <td>0.153057</td>
      <td>0.077893</td>
      <td>0.228221</td>
      <td>0.038350</td>
      <td>49.009848</td>
      <td>3.991084</td>
      <td>2.195726e-04</td>
      <td>***</td>
    </tr>
    <tr>
      <th>IV1</th>
      <td>0.800550</td>
      <td>0.757763</td>
      <td>0.843338</td>
      <td>0.021831</td>
      <td>2477.919857</td>
      <td>36.670958</td>
      <td>1.443103e-235</td>
      <td>***</td>
    </tr>
    <tr>
      <th>IV2</th>
      <td>0.946433</td>
      <td>0.902223</td>
      <td>0.990644</td>
      <td>0.022557</td>
      <td>2473.820894</td>
      <td>41.957498</td>
      <td>4.899264e-291</td>
      <td>***</td>
    </tr>
    <tr>
      <th>IV3</th>
      <td>0.403981</td>
      <td>0.361007</td>
      <td>0.446954</td>
      <td>0.021926</td>
      <td>2465.536399</td>
      <td>18.424992</td>
      <td>3.968080e-71</td>
      <td>***</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Absolute Error of Coef Recovery: 0.1311098578975485
Sum of Model Standard Errors: 0.10466304433776347
Sum of Model T statistics: 101.04453264690632
</code></pre></div></div>

<h3 id="cluster-robust-models">Cluster-robust models</h3>

<p>Next, let’s evaluate the cluster-robust-error modeling approach. Remember, this involves estimating a single regression model to obtain coefficient estimates, but then applying a correction factor to the SEs, and thereby the t-statistics to adjust our inferences. It looks like our coefficient recovery is about the same as our simple multi-level model above, but our inferences are far more conservative due to the larger standard-errors and smaller t-statistics. In fact, these are even a bit more conservative than the fully specified multi-level model we estimated first.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Fit clustered errors LM
</span><span class="n">lm</span> <span class="o">=</span> <span class="n">Lm</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="n">lm</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">robust</span><span class="o">=</span><span class="s">'cluster'</span><span class="p">,</span><span class="n">cluster</span><span class="o">=</span><span class="s">'Group'</span><span class="p">,</span><span class="n">summarize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">lm</span><span class="p">.</span><span class="n">coefs</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Absolute Error of Coef Recovery: </span><span class="si">{</span><span class="n">diffs</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="n">lm</span><span class="p">.</span><span class="n">coefs</span><span class="p">[</span><span class="s">'Estimate'</span><span class="p">])</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Sum of Model Standard Errors: </span><span class="si">{</span><span class="n">lm</span><span class="p">.</span><span class="n">coefs</span><span class="p">[</span><span class="s">'SE'</span><span class="p">].</span><span class="nb">sum</span><span class="p">()</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Sum of Model T statistics: </span><span class="si">{</span><span class="n">lm</span><span class="p">.</span><span class="n">coefs</span><span class="p">[</span><span class="s">'T-stat'</span><span class="p">].</span><span class="nb">abs</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="dataframe">
<table>
  <thead>
    <tr>
      <th></th>
      <th>Estimate</th>
      <th>2.5_ci</th>
      <th>97.5_ci</th>
      <th>SE</th>
      <th>DF</th>
      <th>T-stat</th>
      <th>P-val</th>
      <th>Sig</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Intercept</th>
      <td>0.153013</td>
      <td>0.075555</td>
      <td>0.230470</td>
      <td>0.038481</td>
      <td>46</td>
      <td>3.976365</td>
      <td>2.453576e-04</td>
      <td>***</td>
    </tr>
    <tr>
      <th>IV1</th>
      <td>0.802460</td>
      <td>0.712104</td>
      <td>0.892815</td>
      <td>0.044888</td>
      <td>46</td>
      <td>17.876851</td>
      <td>0.000000e+00</td>
      <td>***</td>
    </tr>
    <tr>
      <th>IV2</th>
      <td>0.945528</td>
      <td>0.851538</td>
      <td>1.039518</td>
      <td>0.046694</td>
      <td>46</td>
      <td>20.249556</td>
      <td>0.000000e+00</td>
      <td>***</td>
    </tr>
    <tr>
      <th>IV3</th>
      <td>0.405163</td>
      <td>0.313841</td>
      <td>0.496485</td>
      <td>0.045368</td>
      <td>46</td>
      <td>8.930504</td>
      <td>1.307510e-11</td>
      <td>***</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Absolute Error of Coef Recovery: 0.13278703905990247
Sum of Model Standard Errors: 0.17543089877808005
Sum of Model T statistics: 51.03327490657406
</code></pre></div></div>

<h3 id="two-stage-regression">Two-stage-regression</h3>

<p>Lastly, let’s use the two-stage-regression approach. We’ll fit a separate regression to each of our 50 people and then compute another regression on those 50 coefficients. In this simple example, we’re really just computing a one-sample t-test on these 50 coefficients. Notice that our coefficient recovery is a tiny bit better than our fully-specified multi-level model and our inferences (based on T-stats and SEs) would largely be similar. This suggests that for this particular dataset we could have gone with either strategy and walked away with the same inference.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Fit two-stage OLS
</span><span class="n">lm2</span> <span class="o">=</span> <span class="n">Lm2</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="n">group</span><span class="o">=</span><span class="s">'Group'</span><span class="p">)</span>
<span class="n">lm2</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">summarize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">lm2</span><span class="p">.</span><span class="n">coefs</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Absolute Error of Coef Recovery: </span><span class="si">{</span><span class="n">diffs</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="n">lm2</span><span class="p">.</span><span class="n">coefs</span><span class="p">[</span><span class="s">'Estimate'</span><span class="p">])</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Sum of Model Standard Errors: </span><span class="si">{</span><span class="n">lm2</span><span class="p">.</span><span class="n">coefs</span><span class="p">[</span><span class="s">'SE'</span><span class="p">].</span><span class="nb">sum</span><span class="p">()</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Sum of Model T statistics: </span><span class="si">{</span><span class="n">lm2</span><span class="p">.</span><span class="n">coefs</span><span class="p">[</span><span class="s">'T-stat'</span><span class="p">].</span><span class="nb">abs</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="dataframe">
<table>
  <thead>
    <tr>
      <th></th>
      <th>Estimate</th>
      <th>2.5_ci</th>
      <th>97.5_ci</th>
      <th>SE</th>
      <th>DF</th>
      <th>T-stat</th>
      <th>P-val</th>
      <th>Sig</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>(Intercept)</th>
      <td>0.144648</td>
      <td>0.070338</td>
      <td>0.218958</td>
      <td>0.036978</td>
      <td>49</td>
      <td>3.911745</td>
      <td>2.822817e-04</td>
      <td>***</td>
    </tr>
    <tr>
      <th>IV1</th>
      <td>0.796758</td>
      <td>0.716781</td>
      <td>0.876736</td>
      <td>0.039798</td>
      <td>49</td>
      <td>20.019944</td>
      <td>0.000000e+00</td>
      <td>***</td>
    </tr>
    <tr>
      <th>IV2</th>
      <td>0.971252</td>
      <td>0.878498</td>
      <td>1.064005</td>
      <td>0.046156</td>
      <td>49</td>
      <td>21.042892</td>
      <td>0.000000e+00</td>
      <td>***</td>
    </tr>
    <tr>
      <th>IV3</th>
      <td>0.424135</td>
      <td>0.339132</td>
      <td>0.509138</td>
      <td>0.042299</td>
      <td>49</td>
      <td>10.027041</td>
      <td>1.840750e-13</td>
      <td>***</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Absolute Error of Coef Recovery: 0.09216204521686983
Sum of Model Standard Errors: 0.16523098907361963
Sum of Model T statistics: 55.00162203260664
</code></pre></div></div>

<h1 id="simulating-a-universe">Simulating a universe.</h1>

<p>Now, this was only one particular dataset with a particular size and particular level of between-person variability. Remember the dimensions outlined above? The real question we want to answer is how these different modeling strategies vary with respect to each of those dimensions. So let’s expand our simulation here. Let’s generate a “grid” of settings such that we simulate every combination of dimensions we can in a reasonable amount of time. Here’s the grid we’ll try to simulate:</p>

<p><img src="/assets/images/compare_rfx/2019-02-18-compare_rfxlm_robustlm_and_lmm_params.png" alt="png" /></p>

<p>Going down the rows we’ll vary <em>dimension 1</em> the sample size of the units we’re making inferences over (number of people) from 5 -&gt; 100. Going across columns we’ll vary <em>dimension 2</em>, the sample size of the units nested within the units we’re making inferences over (number of observations per person) from 5 -&gt; 100. Going over the z-plane we’ll vary <em>dimension 3</em> the variance between the units we’re making inferences over (between-person variability) from 0.10 -&gt; 4 standard deviations.</p>

<p>Since varying <em>dimension 1</em> and <em>dimension 2</em> should make intuitive sense (they’re different aspects of the sample size of our data) let’s explore what varying <em>dimension 3</em> looks like. Here are plots illustrating how changing our between person variance influences coefficients. Each figure below depicts a distribution of <em>person level coefficients</em>; these are the BLUPs we discussed above. When simulating a dataset with two parameters described by an intercept and a slope (IV1), notice how each distribution is centered on the true value of the parameter, but the width of the distribution increases as we increase the between-group variance. These distributions <em>are</em> the distributions that our person level parameters come from. So while they average out to be the same value, they are increasingly dispersed around that value. As these distributions become wider it becomes more challenging to recover the true coefficients of the data if a dataset is too small, as models need more data in order to stabilize their estimates.</p>

<p><span style="font-size: 0.85em; font-style: italic">For the sake of brevity I’ve removed the plotting code for the figures below, but am happy to share them on request.</span></p>

<p><img src="/assets/images/compare_rfx/2019-02-18-compare_rfxlm_robustlm_and_lmm_11_0.png" alt="png" /></p>

<h2 id="setting-it-up">Setting it up</h2>

<p>The next code block sets up this parameter grid and defines some helper functions to compute the metrics defined above. Since this simulation took about ~50 minutes to run on a 2015 quad-core Macbook Pro, I also defined some functions to save each simulation to a csv file.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define the parameter grid
</span><span class="n">nsim</span> <span class="o">=</span> <span class="mi">50</span> <span class="c1"># Number of simulated datasets per parameter combination
</span><span class="n">num_grps</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span> <span class="c1"># Number of "clusters" (i.e. people)
</span><span class="n">obs_grp</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span> <span class="c1"># Number of observations per "cluster"
</span><span class="n">grp_sigmas</span> <span class="o">=</span> <span class="p">[.</span><span class="mi">1</span><span class="p">,</span> <span class="p">.</span><span class="mi">25</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">]</span> <span class="c1"># Between "cluster" variance
</span><span class="n">num_coef</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># Number of terms in the regression equation
</span><span class="n">noise_params</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># Assume each cluster has normally distributed noise
</span><span class="n">seed</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># to repeat this simulation
</span><span class="n">formula</span> <span class="o">=</span> <span class="s">'DV ~ IV1 + IV2 + IV3'</span> <span class="c1"># The model formula
</span>
<span class="c1"># Define some helper functions. diffs() was used above examining each model in detail
</span><span class="k">def</span> <span class="nf">diffs</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="s">"""Absolute error"""</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">calc_model_err</span><span class="p">(</span><span class="n">model_type</span><span class="p">,</span> <span class="n">formula</span><span class="p">,</span> <span class="n">betas</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
    <span class="s">"""
    Fit a model type to data using pymer4. Return the absolute error of the model's
    coefficients, the sum of the model's standard errors, and the sum of the model's
    t-statistics. Also log if the model failed to converge in the case of lme4.
    """</span>

    <span class="k">if</span> <span class="n">model_type</span> <span class="o">==</span> <span class="s">'lm'</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Lm</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
        <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">robust</span><span class="o">=</span><span class="s">'cluster'</span><span class="p">,</span><span class="n">cluster</span><span class="o">=</span><span class="s">'Group'</span><span class="p">,</span><span class="n">summarize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">model_type</span> <span class="o">==</span> <span class="s">'lmer'</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Lmer</span><span class="p">(</span><span class="n">formula</span> <span class="o">+</span> <span class="s">'+ (IV1 + IV2 + IV3 | Group)'</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
        <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">summarize</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">no_warnings</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">model_type</span> <span class="o">==</span> <span class="s">'lmer_mis'</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Lmer</span><span class="p">(</span><span class="n">formula</span> <span class="o">+</span> <span class="s">'+ (1 | Group)'</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
        <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">summarize</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">no_warnings</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">model_type</span> <span class="o">==</span> <span class="s">'lm2'</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Lm2</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span><span class="n">group</span><span class="o">=</span><span class="s">'Group'</span><span class="p">)</span>
        <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">summarize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="n">coef_diffs</span> <span class="o">=</span> <span class="n">diffs</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">coefs</span><span class="p">[</span><span class="s">'Estimate'</span><span class="p">])</span>
    <span class="n">model_ses</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">coefs</span><span class="p">[</span><span class="s">'SE'</span><span class="p">].</span><span class="nb">sum</span><span class="p">()</span>
    <span class="n">model_ts</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">coefs</span><span class="p">[</span><span class="s">'T-stat'</span><span class="p">].</span><span class="nb">abs</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">warnings</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">warnings</span> <span class="o">==</span> <span class="p">[]):</span>
        <span class="n">model_success</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">model_success</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="k">return</span> <span class="n">coef_diffs</span><span class="p">,</span> <span class="n">model_ses</span><span class="p">,</span> <span class="n">model_ts</span><span class="p">,</span> <span class="n">model_success</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">coefs</span>

<span class="k">def</span> <span class="nf">save_results</span><span class="p">(</span><span class="n">err_params</span><span class="p">,</span> <span class="n">sim_params</span><span class="p">,</span> <span class="n">sim</span><span class="p">,</span> <span class="n">model_type</span><span class="p">,</span> <span class="n">model_coefs</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">coef_df</span><span class="p">,</span> <span class="n">save</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>

    <span class="s">"""Aggregate and save results using pandas"""</span>

    <span class="n">model_coefs</span><span class="p">[</span><span class="s">'Sim'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sim</span>
    <span class="n">model_coefs</span><span class="p">[</span><span class="s">'Model'</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_type</span>
    <span class="n">model_coefs</span><span class="p">[</span><span class="s">'Num_grp'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sim_params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">model_coefs</span><span class="p">[</span><span class="s">'Num_obs_grp'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sim_params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">model_coefs</span><span class="p">[</span><span class="s">'Btwn_grp_sigma'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sim_params</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

    <span class="n">coef_df</span> <span class="o">=</span> <span class="n">coef_df</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">model_coefs</span><span class="p">)</span>

    <span class="n">dat</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span>
            <span class="s">'Model'</span><span class="p">:</span> <span class="n">model_type</span><span class="p">,</span>
            <span class="s">'Num_grp'</span><span class="p">:</span> <span class="n">sim_params</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="s">'Num_obs_grp'</span><span class="p">:</span> <span class="n">sim_params</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="s">'Btwn_grp_sigma'</span><span class="p">:</span> <span class="n">sim_params</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
            <span class="s">'Coef_abs_err'</span><span class="p">:</span> <span class="n">err_params</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="s">'SE_sum'</span><span class="p">:</span> <span class="n">err_params</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="s">'T_sum'</span><span class="p">:</span> <span class="n">err_params</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
            <span class="s">'Fit_success'</span><span class="p">:</span> <span class="n">err_params</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
            <span class="s">'Sim'</span><span class="p">:</span> <span class="n">sim</span>
        <span class="p">},</span>  <span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">dat</span><span class="p">,</span><span class="n">ignore_index</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">save</span><span class="p">:</span>
        <span class="n">df</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">'./sim_results.csv'</span><span class="p">,</span><span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">coef_df</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">'./sim_estimates.csv'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span><span class="p">,</span> <span class="n">coef_df</span>

<span class="c1"># Run it
</span><span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">coef_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="s">'lm'</span><span class="p">,</span> <span class="s">'lm2'</span><span class="p">,</span> <span class="s">'lmer'</span><span class="p">,</span> <span class="s">'lmer_mis'</span><span class="p">]</span>
<span class="k">for</span> <span class="n">N</span> <span class="ow">in</span> <span class="n">num_grps</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">O</span> <span class="ow">in</span> <span class="n">obs_grp</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">S</span> <span class="ow">in</span> <span class="n">grp_sigmas</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">I</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nsim</span><span class="p">):</span>
                <span class="n">data</span><span class="p">,</span> <span class="n">blups</span><span class="p">,</span> <span class="n">betas</span> <span class="o">=</span> <span class="n">simulate_lmm</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">num_coef</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">grp_sigmas</span><span class="o">=</span><span class="n">S</span><span class="p">,</span> <span class="n">noise_params</span><span class="o">=</span><span class="n">noise_params</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">M</span> <span class="ow">in</span> <span class="n">models</span><span class="p">:</span>
                    <span class="n">c</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">success</span><span class="p">,</span> <span class="n">coefs</span><span class="p">,</span> <span class="o">=</span> <span class="n">calc_model_err</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">formula</span><span class="p">,</span> <span class="n">betas</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
                    <span class="n">results</span><span class="p">,</span> <span class="n">coef_df</span> <span class="o">=</span> <span class="n">save_results</span><span class="p">([</span><span class="n">c</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">t</span><span class="p">,</span> <span class="n">success</span><span class="p">],</span> <span class="p">[</span><span class="n">N</span><span class="p">,</span><span class="n">O</span><span class="p">,</span><span class="n">S</span><span class="p">],</span> <span class="n">I</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">coefs</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">coef_df</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="results">Results</h1>

<p><span style="font-size: 0.85em; font-style: italic">For the sake of brevity I’ve removed the plotting code for the figures below, but am happy to share them on request!</span></p>

<h2 id="coefficient-recovery">Coefficient Recovery</h2>

<p>Ok, let’s take first take a look at our coefficient recovery. If we look from the top left of the grid to the bottom right the first thing to jump out is that when we increase our overall sample size (number of clusters * number of observations per cluster), and our between cluster variability is medium to low, <em>all</em> model types do a similarly good job of recovering the true data generating coefficients. In other words, under good conditions (lots of data that isn’t too variable) we can’t go wrong picking any of the analysis strategies. In the converse, going from bottom left to top right, when between cluster variability is high, we quickly see the importance of having <em>more clusters</em> rather than more observations per cluster; without enough clusters to observe, even a fully specified multi-level model does a poor job of recovering the true coefficients.</p>

<p>When we have small to medium sized datasets and lots of between-cluster variability all models tend to do a poor job of recovering the true coefficients. Interestingly, having particularly few observations per cluster (left-most column) disproportionately affects two-stage-regression estimation (orange boxplots). This is consistent with <a href="http://www.stat.columbia.edu/~gelman/research/published/459.pdf">Gelman, 2005</a> who suggests that with few per cluster observations, the first-level OLS estimates are pretty poor with high-variance and there are none of the multi-level modeling benefits to help offset the situation. This situation also seems to favor fully-specified multi-level models the most (green boxplots), particularly when between cluster variability is high. It’s interesting to note that cluster-robust, and misspecified (simple) multi-level models seem to perform similarly in this situation.</p>

<p>In medium data situations (middle column) cluster-robust models seem to do a slightly worse job across the board of recovering coefficients. This is most likely due to the fact that the estimates completely ignore the clustered nature of the data and have no smoothing/regularization applied to them either through averaging (in the case of the two-stage-regression models) or through random-effects estimation (in the case of the multi-level models).</p>

<p>Finally, in the high observations per cluster situation (right-most column), all models seem to perform rather similarly suggesting that each modeling strategy is about as good as any other when we densely sampling the unit of interest (increasing number of observations per cluster) even if the desire is to make inferences about the clusters themselves.</p>

<p><img src="/assets/images/compare_rfx/2019-02-18-compare_rfxlm_robustlm_and_lmm_15_0.png" alt="png" /></p>

<h2 id="making-inferences-ses--t-stats">Making Inferences (SEs + T-stats)</h2>

<p>Next, let’s look at both standard errors and t-statistics to see how our inferences might vary. The effect of increased between cluster variance has a very notable effect on SEs and t-stats values generally making it less likely to identify a statistically significant relationship regardless of the size of the data. Interestingly, what two-stage-regression models exhibit in terms of poorer coefficient recovery in situations with few observations per cluster, they make up for with higher standard error estimates. We can see that their t-statistics are low in these situations suggesting that in these situations this approach may tip the scales towards lower false-positive, higher false-negative inferences. However, unlike other model types, they do not necessarily benefit from <em>more clusters</em> overall (bottom left panel) and run the risk of an inflated level of <a href="https://www.wikiwand.com/en/False_positives_and_false_negatives">false negatives</a>. Misspecified multilevel models seem to have the opposite properties: they have higher t-stats and lower SEs in most situations with medium to high between-cluster variability and benefit the most from situations with a high number of observations per cluster. This suggests they might run the risk of introducing more false positives in situations where other models may behave more conservatively, but also may be more sensitive to detecting true relationships in the face of high between-cluster variance. They also seem to benefit most from increasing <em>observations within cluster</em>. Inferences from cluster-robust and full-specified multi-level models seem to be largely comparable which is consistent with the proliferate use of both these model types across multiple literatures.</p>

<p><img src="/assets/images/compare_rfx/2019-02-18-compare_rfxlm_robustlm_and_lmm_16_0.png" alt="png" /></p>

<p><img src="/assets/images/compare_rfx/2019-02-18-compare_rfxlm_robustlm_and_lmm_17_0.png" alt="png" /></p>

<h2 id="bonus-when-fully-specified-multi-level-models-fail">Bonus: When fully-specified multi-level models fail</h2>

<p>Finally, we can take a brief look at what situations most often cause convergence failures for our fully-specified multi-level models (note: the simple multi-level models examined earlier never failed to converge in these simulations). In general, this seems to occur when between cluster variability is low, or the number of observations per cluster is very small. This makes sense because even though the data were generated in a multi-level manner, clusters are quite similar and simplifying models by discarding terms which try to model variance that may not be exhibited by the data in a meaningful way (e.g. dropping “random-slopes”) achieve better estimation overall. In other words, the model may be trying to fit a variance parameter that is small enough to cause it to run out of optimizer iterations before it reaches a suitably small change in error. This is like trying to <a href="https://towardsdatascience.com/gradient-descent-d3a58eb72404">find the lowest point</a> on a “hill” that has a very shallow declining slope by comparing the height of your current step to the height of your previous step.</p>

<p><img src="/assets/images/compare_rfx/2019-02-18-compare_rfxlm_robustlm_and_lmm_18_0.png" alt="png" /></p>

<h2 id="conclusions">Conclusions</h2>

<p>So what have we learned? Here are some intuitions that I think this exercise has helped flesh out:</p>
<ul>
  <li>Reserve two-stage-regression for situations when there are enough observations per cluster. This is because modeling each cluster separately without any type of smoothing/regularization/prior imposed by multi-level modeling, produces poor first-level estimates in these situations.</li>
  <li>Be careful using misspecified/simple multi-level models. While they may remove some of the complexity involved in specifying the “random effects” part of the model, and they converge pretty much all the time, they are more likely to lead to statistically significant inferences relative to other approaches (all else being equal). This may be warranted if your data don’t exhibit enough between-cluster variance. It may be generally preferable then to specify a model structure that accounts for variance confounded with predictors of interest (<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3881361/">Barr et al, 2013</a>) (i.e. dropping the correlation term between a random intercept and random slope rather than dropping the random slope), in other words the most “maximal” structure you can get away with, with respect to the inferences you want to make of your data.</li>
  <li>Cluster-robust models appear to be an efficient solution if your primary goal is making inferences and you can live with coefficient estimates that are a bit less accurate than other approaches. These are harder to specify if there are multiple levels of clustering in the data (e.g. survey responses within-person, within-city, within-state, etc) or if accounting for item-level effects are important (<a href="http://www.sfs.uni-tuebingen.de/~hbaayen/publications/baayenDavidsonBates.pdf">Baayen et al, 2008</a>). However, there are techniques to incorporate two-way or multi-way cluster-robust errors and such approaches are reasonably common in economics. <a href="https://www.stata.com/meeting/boston10/boston10_baum.pdf">This lecture</a> and <a href="https://www.nber.org/papers/t0327.pdf">this paper</a> discuss these approaches further. Pymer4 used for this post only implements one-way clustering.</li>
  <li>Consider using two-stage-regression or cluster-robust errors<sup id="a2"><a href="#f2">1</a></sup> instead of misspecified multi-level models as your inferences maybe be largely similar to fully-specified multi-level models that converge successfully. This may not be true if item-level variance or multiple-levels of clustering need to be taken into account, but for relatively straight forward cases illustrated in this post, they seem to fair just fine.</li>
  <li>Generally, simulations can be helpful ways to build statistical intuitions especially if the background mathematics feels daunting. This is has been one of my preferred approaches for learning statistical concepts in more depth and has made reading literature heavy on mathematical formalisms far more approachable.</li>
</ul>

<h3 id="caveats-and-cautions">Caveats and cautions</h3>

<p>I don’t want to end this post with the feeling that we’ve figured everything out and are expert analysts now, but rather appreciate that there are some limitations to this exercise that are worth keeping in mind. While we can build some general intuitions, there are conditions under which these intuitions may not always hold and it’s incredibly important to be aware of them:</p>
<ul>
  <li>In many ways, the data generated in these simulations were “ideal” for playing around with these different approaches. Data points were all on the same scale, came from a normal distribution with known means and variances, contained no missing data points, and adhered to other underlying assumptions of these statistical approaches not discussed.</li>
  <li>Similarly, I built-in “real relationships” into the data and then tried to recover them. I made mention of false-positives and negatives throughout the post, but I did not  formally estimate the false-positive or false-negative rate for any of these approaches. Again this was by design to leave you with some general intuitions, but there exist several papers that utilize this approach to more explicitly defend the use of certain inference techniques (e.g. <a href="https://www.ncbi.nlm.nih.gov/pubmed/27620283">Luke, 2017</a>).</li>
  <li>The space of parameters we explored (i.e. the different “dimensions” of our data) spanned a range I thought reasonably covered a variety of datasets often collected in empirical social science laboratory studies. In the real world, data are far messier and increasingly, far larger. More data is almost always better, particularly if it’s of high quality, but what constitutes quality can be very different based on the inferences one wants to make. Sometimes high variability between clusters is desirable, other times densely sampling a small set of clusters is more important. These factors will vary based on the questions one is trying to answer.</li>
  <li>The metrics I chose to evaluate each model with are simply the ones that I wanted to know about. There are certainly other metrics that could be more or less informative based on what intuitions you would like to build. For example, what is the prediction accuracy of each model?</li>
</ul>

<h1 id="conclusion">Conclusion</h1>

<p>I hope this was useful for some folks out there and even if it wasn’t, it certainly helped <em>me</em> build some intuitions about the different analysis strategies that are available. Moreover, I hope if nothing else, this might motivate people who feel like they have limited formal training in statistics/machine-learning to take a more tinker/hacker approach to their own learning. I remember when as a kid, breaking things and taking them apart was one of my favorite ways to learn about how things worked. With the mass availability of free and open-source tools like scientific Python and R, I see no reason why statistical education can’t be the same.</p>

<h2 id="appendix">Appendix</h2>

<p>This is a <a href="https://www.reed.edu/economics/parker/s11/312/notes/Notes13.pdf">nice quick guide</a> that defines much of the terminology across different fields and reviews a lot of the concepts covered here (plus more) in a much more pithy way. For those interested, p-values for multi-level models were computed using the <a href="https://cran.r-project.org/web/packages/lmerTest/index.html">lmerTest R package</a> using Satterthwaite approximation for degrees of freedom calculation; note that based on the random-effects structure specified, these degrees of freedom can change dramatically. P-values for other model types were computed using a standard t-distribution, but <a href="http://eshinjolly.com/pymer4/usage.html">pymer4 also offers</a> non-parametric permutation testing and boot-strapped confidence intervals for other styles of inference. At the time of this writing, fitting two-stage-regression models is only available in <a href="https://github.com/ejolly/pymer4/tree/dev">development branch on github</a>, but should be incorporated in a new release in the future.</p>

<h2 id="notes-and-corrections"><em>Notes and Corrections</em></h2>

<p><b id="f1">*</b><em>In a previous version of this post, this approach was mistakenly called two-stage-least-squares (2SLS). 2SLS is a formal name for a completely different technique which falls under the broader scope of <a href="https://www.wikiwand.com/en/Instrumental_variables_estimation#/Interpretation_as_two-stage_least_squares">instrumental variable estimation</a>. This confusion is because the two-stage-regression approach discussed above technically does employ “two-stages” of ordinary-least-squares estimation, yet this is not what 2SLS is in the literature. Thanks to <a href="http://jonasobleser.com/">Jonas Oblesser</a> for pointing this out and <a href="http://www.senns.demon.co.uk/home.html">Stephen John Senn</a> for appropriate terminology that is in fact consistent across medical and fMRI literatures.</em><a href="#a1">↩</a></p>

<p><span id="f2">1.</span> While in this post (and often in the literature) two-stage, or even multi-level modeling and cluster-robust inference are seen as two <em>different</em> possible analytic strategies another possibility involves <em>combining</em> these approaches. That is, using a multi-level model or two-stage-regression to obtain coefficient estimates, and then computing robust-standard errors on the highest-level coefficients when performing inference. Thanks to <a href="https://www.jepusto.com/">James E. Pustejovsky</a> for bringing up this often overlooked option.<a href="#a2">↩</a></p>]]></content><author><name></name></author><category term="statistics" /><category term="analysis" /><category term="simulation" /><summary type="html"><![CDATA[Dealing with dependencies in data]]></summary></entry><entry><title type="html">Predicting wait lines using computer vision and machine learning</title><link href="https://eshinjolly.com/2019/02/11/line_at_kaf/" rel="alternate" type="text/html" title="Predicting wait lines using computer vision and machine learning" /><published>2019-02-11T00:00:00-05:00</published><updated>2019-02-11T00:00:00-05:00</updated><id>https://eshinjolly.com/2019/02/11/line_at_kaf</id><content type="html" xml:base="https://eshinjolly.com/2019/02/11/line_at_kaf/"><![CDATA[<div class="flex w-1/2 mx-auto gap-x-2">
  <img src="/assets/images/lineatkaf/main.png" class="w-3/5" />
  <img src="/assets/images/kaf_people.png" class="w-2/5" />
</div>
<figcaption class="text-sm italic text-center">This blog post was co-written with Jin.</figcaption>

<p>For the past year I’ve been working on the Line@KAF project with <a href="http://jinhyuncheong.com/">Jin Cheong</a> with the support from <a href="http://cosanlab.com">COSAN lab</a>, <a href="http://dali.dartmouth.edu/projects-blog/kaf">DALI lab</a>, and <a href="https://magnuson.dartmouth.edu/">Magnuson Center of Entrepreneurship</a>.</p>

<p>Line@KAF allows real-time tracking of the length of the queue at KAF, a popular cafe at Dartmouth College. Users can browse the menu and prices, see data on popular times by hour, and submit their requests and feedback to the KAF team!</p>

<p>While we are making finishing touches to the iOS and Android apps, we launched a beta version of the app
on our website at <a href="http://lineatkaf.com/">http://lineatkaf.com/</a>.
Please visit our website to take a sneak peek at our app and send us your thoughts and suggestions.</p>

<div class="grid w-3/4 grid-cols-2 gap-4 mx-auto">
    <img src="/assets/images/lineatkaf/img1.png" class="my-0" />    
    <img src="/assets/images/lineatkaf/img2.png" class="my-0" />  
    <img src="/assets/images/lineatkaf/img3.png" class="my-0" />  
    <img src="/assets/images/lineatkaf/img4.png" class="my-0" />  
</div>

<p>To make our app possible, we use a combination of tools including computer vision algorithms, neural network models, and cloud computing services. We’ll write a more detailed post of how it works in the future but here is a quick infographic overview of the system.</p>

<figure>
    <img src="/assets/images/lineatkaf/infographic.png" class="w-3/4 mx-auto" />    
</figure>

<p>While we are currently testing our services at the King Arthur Flour cafe located at Baker-Berry library,
we look forward to extending our services to multiple locations on campus and in the community where students,
customers, and businesses can all benefit from knowing the line.</p>]]></content><author><name></name></author><category term="machine-learning" /><category term="web app" /><category term="project" /><summary type="html"><![CDATA[Real-time queue tracking]]></summary></entry><entry><title type="html">Your favorite time-machine</title><link href="https://eshinjolly.com/2019/01/04/git_github/" rel="alternate" type="text/html" title="Your favorite time-machine" /><published>2019-01-04T00:00:00-05:00</published><updated>2019-01-04T00:00:00-05:00</updated><id>https://eshinjolly.com/2019/01/04/git_github</id><content type="html" xml:base="https://eshinjolly.com/2019/01/04/git_github/"><![CDATA[<p>This is a long overdue post regarding one of the tutorials I taught at the 2018 Dartmouth Methods in Neuroscience (MIND) summer school on version control for researchers. Rather than recount my 30 minute talk and demo here, I figured I would share direct links to the materials so interested folks could peruse them at their leisure.</p>

<p>In this talk, and for myself, I’ve found the analogy of a version control system (e.g. git) as a (social) time-machine incredibly useful for understanding fundamental concepts. In case this framing is useful for others, my talk can be seen in it’s entirety at the bottom of this post; the slides accompanying the talk can be found <a href="https://github.com/Summer-MIND/mind_2018/blob/master/slides/git_github_slides.pdf">here</a>.</p>

<p>Something summer school attendees told me they found really useful were some animated gifs I generated capturing the output of the most commonly used git commands, as well as actions on github.com, to see what to expect and how they work. I’ve embedded those below as a reference as well. Enjoy!</p>

<h2 id="git-status"><code class="language-plaintext highlighter-rouge">git status</code></h2>
<p>See what files are ready to be made into a “snapshot” (committed) and which ones are not being kept track of<br />
<img src="/assets/images/example_command_gifs/gitstatus.gif" alt="git status" /></p>

<h2 id="git-init"><code class="language-plaintext highlighter-rouge">git init</code></h2>
<p>Create a new git repository for the first time (will not add any files)<br />
<img src="/assets/images/example_command_gifs/gitinit.gif" alt="git init" /></p>

<h2 id="git-add"><code class="language-plaintext highlighter-rouge">git add</code></h2>
<p>Add file(s) to the list of files that should be made into a “snapshot” (committed)
<img src="/assets/images/example_command_gifs/gitadd.gif" alt="git add" /></p>

<h2 id="git-commit"><code class="language-plaintext highlighter-rouge">git commit</code></h2>
<p>Take a “snapshot” of all currently tracked project files. Files need to be “prepped” (staged) for commit using <code class="language-plaintext highlighter-rouge">git add</code> beforehand.  <br />
<img src="/assets/images/example_command_gifs/gitcommit.gif" alt="git commit" /></p>

<h2 id="git-log"><code class="language-plaintext highlighter-rouge">git log</code></h2>
<p>See the full historical timeline of the project<br />
<img src="/assets/images/example_command_gifs/gitlog.gif" alt="git log" /></p>

<h2 id="git-push"><code class="language-plaintext highlighter-rouge">git push</code></h2>
<p>Send latest local changes to a remote location (e.g. github)<br />
<img src="/assets/images/example_command_gifs/gitpush.gif" alt="git push" /></p>

<h2 id="git-pull"><code class="language-plaintext highlighter-rouge">git pull</code></h2>
<p>Get the latest changes from a remote location (e.g. github)<br />
<img src="/assets/images/example_command_gifs/gitpull.gif" alt="git pull" /></p>

<h2 id="git-clone"><code class="language-plaintext highlighter-rouge">git clone</code></h2>
<p>Duplicate a remote repository (e.g. github) on your local computer<br />
<img src="/assets/images/example_command_gifs/gitclone.gif" alt="git clone" /></p>

<h2 id="git-branch"><code class="language-plaintext highlighter-rouge">git branch</code></h2>
<p>Create a new independent “timeline” for the project<br />
<img src="/assets/images/example_command_gifs/gitbranch.gif" alt="git branch" /></p>

<h2 id="git-revert"><code class="language-plaintext highlighter-rouge">git revert</code></h2>
<p>Undo changes by reversing any specific “snapshot” (commit)<br />
<img src="/assets/images/example_command_gifs/gitrevert.gif" alt="git revert" /></p>

<h2 id="forking"><code class="language-plaintext highlighter-rouge">forking</code></h2>
<p>Copy a remote repository on github, to your own remote account on github<br />
<img src="/assets/images/example_command_gifs/gitfork.gif" alt="git fork" /></p>

<h2 id="pull-request"><code class="language-plaintext highlighter-rouge">pull request</code></h2>
<p>Notify a remote repository owner you would like them to review+incorporate your additions<br />
<img src="/assets/images/example_command_gifs/pullrequest.gif" alt="pull request" /></p>

<hr />
<iframe class="mx-auto" src="https://www.youtube.com/embed/0DGCnBZBoc0" width="600" height="400"></iframe>]]></content><author><name></name></author><category term="tutorial" /><category term="software" /><category term="hackathon" /><category term="reference" /><summary type="html"><![CDATA[Understanding version control]]></summary></entry><entry><title type="html">Vector similarity reference</title><link href="https://eshinjolly.com/2018/04/12/similarity_metrics/" rel="alternate" type="text/html" title="Vector similarity reference" /><published>2018-04-12T00:00:00-04:00</published><updated>2018-04-12T00:00:00-04:00</updated><id>https://eshinjolly.com/2018/04/12/similarity_metrics</id><content type="html" xml:base="https://eshinjolly.com/2018/04/12/similarity_metrics/"><![CDATA[<p>This is a quick post to illustrate with python code how several common vector similarity computations are related to each other. For more details I highly encourage you to check out Brendan O’Connor’s really nice <a href="https://brenocon.com/blog/2012/03/cosine-similarity-pearson-correlation-and-ols-coefficients/">elaboration</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Import some stuff
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">scipy.spatial.distance</span> <span class="k">as</span> <span class="n">spd</span>
<span class="kn">from</span> <span class="nn">pymer4.simulate</span> <span class="kn">import</span> <span class="n">easy_multivariate_normal</span>
<span class="kn">from</span> <span class="nn">pymer4.models</span> <span class="kn">import</span> <span class="n">Lm</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simulate some data
# 2, 50 dimensional vectors correlated ~ r = -0.05
</span><span class="n">X</span> <span class="o">=</span> <span class="n">easy_multivariate_normal</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">corrs</span><span class="o">=-</span><span class="p">.</span><span class="mi">05</span><span class="p">)</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div>

<h2 id="inner-product">Inner product</h2>
<p><strong>Sum of element wise multiplication</strong></p>

\[\sum_i {x_iy_i} = x \cdot y\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
<span class="o">-</span><span class="mf">2.8071010155482368</span>
</code></pre></div></div>

<h2 id="covariance">Covariance</h2>
<p><strong>Average centered inner product</strong></p>

\[\frac{(x - \bar x) \cdot (y - \bar y)}{n}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a_centered</span> <span class="o">=</span> <span class="n">a</span> <span class="o">-</span> <span class="n">a</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">b_centered</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">b</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a_centered</span><span class="p">,</span><span class="n">b_centered</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="o">-</span><span class="mf">0.06920945007922867</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Check our work
</span><span class="n">np</span><span class="p">.</span><span class="n">cov</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
<span class="o">-</span><span class="mf">0.06920945007922867</span>
</code></pre></div></div>

<h2 id="cosine-similarity">Cosine Similarity</h2>
<p><strong>Normalized (L2) inner product</strong></p>

\[\frac{x \cdot y}{||x|| \ ||y||}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Euclidean/L2 norm = square root of sum of squared values
# algebra form
</span><span class="n">a_norm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">power</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="mi">2</span><span class="p">)))</span>
<span class="c1"># matrix form; transpose is not strictly needed here, just for illustration
</span><span class="n">b_norm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="n">b</span><span class="p">.</span><span class="n">T</span><span class="p">))</span>

<span class="c1"># numpy short-cut: np.linalg.norm(a)
</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">a_norm</span> <span class="o">*</span> <span class="n">b_norm</span><span class="p">)</span>
<span class="o">-</span><span class="mf">0.049727707335311774</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Check our work (subtract 1 because scipy returns distances)
</span><span class="mi">1</span> <span class="o">-</span> <span class="n">spd</span><span class="p">.</span><span class="n">cosine</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
<span class="o">-</span><span class="mf">0.04972770733531173</span>
</code></pre></div></div>

<h2 id="pearson-correlation">Pearson Correlation</h2>
<p><strong>Centered, normalized (L2) inner product</strong></p>

\[\frac{(x - \bar x) \cdot (y - \bar y)}{||x - \bar x|| \ ||y - \bar y||}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Can think of this as normalized covariance OR centered cosine similarity
</span><span class="n">a_centered_norm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a_centered</span><span class="p">)</span>
<span class="n">b_centered_norm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">b_centered</span><span class="p">)</span>
<span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a_centered</span><span class="p">,</span><span class="n">b_centered</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">a_centered_norm</span> <span class="o">*</span> <span class="n">b_centered_norm</span><span class="p">)</span>
<span class="o">-</span><span class="mf">0.06299972596364047</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Check our work
</span><span class="mi">1</span> <span class="o">-</span> <span class="n">spd</span><span class="p">.</span><span class="n">correlation</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
<span class="o">-</span><span class="mf">0.06299972596364034</span>
</code></pre></div></div>

<h2 id="ols-univariate-wo-intercept">OLS (univariate w/o intercept)</h2>
<p><strong>Partially normalized inner product, where partially means using a single vector’s norm</strong></p>

\[\frac{x \cdot y}{||x||^2}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Can think of this as cosine similarity using only one vector's norm
</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">a_norm</span> <span class="o">*</span> <span class="n">a_norm</span><span class="p">)</span>
<span class="o">-</span><span class="mf">0.06164964567554537</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Check our work against a regression in pymer4
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Lm</span><span class="p">(</span><span class="s">'B ~ 0 + A'</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'A'</span><span class="p">:</span><span class="n">a</span><span class="p">,</span><span class="s">'B'</span><span class="p">:</span><span class="n">b</span><span class="p">}))</span>
<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">summarize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="c1"># Grab the beta value directly from the coefficients table.
</span><span class="n">model</span><span class="p">.</span><span class="n">coefs</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="o">-</span><span class="mf">0.06164964567554537</span>
</code></pre></div></div>

<h2 id="ols-univariate-w-intercept">OLS (univariate w/ intercept)</h2>
<p><strong>Centered, partially normalized inner product</strong></p>

\[\frac{(x - \bar x) \cdot y}{||x - \bar x||^2}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># In the numerator we could actually center a or b, or both.
</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a_centered</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">a_centered_norm</span> <span class="o">*</span> <span class="n">a_centered_norm</span><span class="p">)</span>
<span class="o">-</span><span class="mf">0.07620054357106859</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Check our work
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Lm</span><span class="p">(</span><span class="s">'B ~ A'</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'A'</span><span class="p">:</span><span class="n">a</span><span class="p">,</span><span class="s">'B'</span><span class="p">:</span><span class="n">b</span><span class="p">}))</span>
<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">summarize</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">coefs</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="o">-</span><span class="mf">0.07620054357106855</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="statistics" /><category term="reference" /><summary type="html"><![CDATA[How different similarity measures are related to each other]]></summary></entry><entry><title type="html">Analysis containers and compute clusters</title><link href="https://eshinjolly.com/2017/06/03/singularity-intro/" rel="alternate" type="text/html" title="Analysis containers and compute clusters" /><published>2017-06-03T00:00:00-04:00</published><updated>2017-06-03T00:00:00-04:00</updated><id>https://eshinjolly.com/2017/06/03/singularity-intro</id><content type="html" xml:base="https://eshinjolly.com/2017/06/03/singularity-intro/"><![CDATA[<p>Recently <a href="http://pbs.dartmouth.edu/">Dartmouth graduate students</a> collectively organized a full day of workshops to share and help each other out with the plethora of data analysis tools we regularly use in our respective research endeavors. Amongst these was a brief hands-on tutorial I gave with <a href="http://mvdoc.me/">Matteo Visconti</a> on utilizing container-based analysis environment with <a href="http://techdoc.dartmouth.edu/discovery/">Discovery</a>, Dartmouth’s high-performance compute cluster (HPC).</p>

<p>For a nice primer on what containers are and how they can be useful for sharing and reproducing research I highly recommend checking out Matteo’s <a href="http://mvdoc.me/2017/using-singularity-to-make-analyses-reproducible.html">blog post</a>. The general idea is to create a portable computing environment that has all the tools one might need to conduct a particular analysis workflow (i.e. the actual programs and system architecture that your analysis scripts can be executed with).</p>

<p>While tools like <a href="https://www.docker.com/">Docker</a> have been especially popular in the software development world for this purpose, utilizing them in a computing environment where one does not have root privileges (e.g. a university HPC maintained by system administrators) is largely not possible. <a href="http://singularity.lbl.gov/">Singularity</a> was developed for exactly this purpose: the benefits of a containerized analysis environment, that <em>can</em> be utilized on a HPC. Because getting setup can be a bit tricky, our tutorial provides basic HPC users (particularly those with personal computers running OSX) with the steps required to get going with Singularity. Just follow the link below. Enjoy.</p>

<div style="text-align: center; font-size: 2em"><a class="nounderline" href="https://github.com/ejolly/IntroToSingularity">Intro to Singularity</a>
</div>]]></content><author><name></name></author><category term="tutorial" /><category term="analysis" /><summary type="html"><![CDATA[Using containers on a compute cluster for reproducible science]]></summary></entry><entry><title type="html">My Love Affair with Jupyter Notebooks</title><link href="https://eshinjolly.com/2017/03/02/jupyter-intro/" rel="alternate" type="text/html" title="My Love Affair with Jupyter Notebooks" /><published>2017-03-02T00:00:00-05:00</published><updated>2017-03-02T00:00:00-05:00</updated><id>https://eshinjolly.com/2017/03/02/jupyter-intro</id><content type="html" xml:base="https://eshinjolly.com/2017/03/02/jupyter-intro/"><![CDATA[<p>This year I helped organize the first annual <a href="https://dartmouthbrainhack.github.io/">Brainhack</a> at Dartmouth College. It was our satellite hackathon, which was part of the larger annual <a href="http://events.brainhack.org/global2017/">Brainhack Global</a>. Overall we received a lot of positive feedback and attendees said they learned a ton.</p>

<p>In addition to being an organizer I gave a brief introduction to <a href="http://jupyter.org/">Jupyter notebooks</a> as an environment for interactive data analysis (and much more). There are already a ton of great resources explaining notebook features in detail, so for this talk I wanted to provide a brief overview and discuss how I most often use notebooks: as a highly customizable, one-stop, shareable, multi-language, analysis environment.<br />
<em>Bonus: The slides themselves are generated using a Jupyter Notebook</em></p>

<iframe src="https://nbviewer.jupyter.org/format/slides/github/dartmouth-brainhack-2017/IntroToJupyter/blob/master/Intro_To_Jupyter_Notebooks.ipynb#/" width="800" height="600"></iframe>]]></content><author><name></name></author><category term="tutorial" /><category term="analysis" /><category term="python" /><category term="scientific-tooling" /><summary type="html"><![CDATA[Introduction to Jupyter Notebooks]]></summary></entry></feed>